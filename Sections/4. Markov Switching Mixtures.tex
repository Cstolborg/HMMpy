\newpage
 \section{Data model}
As highlighted throughout section \ref{section: Data}, a traditional Gaussian distribution provides a poor fit in regards to financial return series. In addition, when applying a variety of different predictive techniques to financial return series most of the models assume stationarity. However, Hamilton (1989) found that financial returns are often characterised as non-stationary across time due to changing means and heteroscedasticity. It should be noted that despite the fact that stationarity might not be fulfilled across long time horizons, the condition is often fulfilled over temporary periods (Hamilton, 1989). Instead mixtures of Gaussian distributions provide an enhanced fit, since these distributions are better at reproducing the aforementioned stylized facts both in terms of leptokurtosis and skewness. Ryden et al. (1998) found HMM's suitable for reproducing stylized facts on 10 different subsets of approximately 1,700 observations and Nystrup et al. (2017) expanded these findings by considering a rolling window of equal length from the 1930s until the 2010s.

The fundamental aspect of an HMM is that the underlying distribution that generates an observation is dependent on the state generated by an unobserved Markov chain. In addition, the transition probabilities, which determines the probability of which a state change occurs, are assumed to be constant, thereby implying that the sojourn times, i.e. the amount of time spend in each state, are geometrically distributed. However, since the thesis is centered around modelling of economic regimes, and economic theory prescribe that a minor or major recession on average occurs every seven years, the memoryless property of the geometric distribution is not always appropriate. The limitation is somewhat circumvented by relying on global stock indices, such as the MSCI World, as well as by setting a limitation on the variation of the amount of different economic regimes. An alternative approach would be to model the economic regimes thorough a Hidden Semi Markov Model (HSMM), which allows for an explicit modelling of the sojourn time distributions. 

The theory of HMMs in discrete time as well as their associated mathematics and estimation is outlined through section \ref{subsection: HMM} to \ref{subsection: Decoding}. In addition, the thesis will be exploring a model estimation method, first noted by Nystrup et. al (2020) in which the jump framework of Bemporad et al. (2018) is combined with the temporal features used by Zheng et al. (2019), in order to estimate HMMs. This methodology is known as the \textit{jump estimator} and will be reviewed in section \ref{subsection: Jump theory}. As such, the jump framework serves as an alternative to the classical maximum likelihood estimation (MLE) approach. 



\subsection{Hidden Markov Models}
\label{subsection: HMM}
Hidden Markov Models, are probabilistic models whose primary function is to predict sequences of unobserved (hidden) states from a set of observed variables. The observed variables could be a price series of a financial asset like the MSCI World index.

The model’s inference of hidden states is based on an underlying Markov process. As such, as opposed to other traditional model which assume independence among observations, Markov models assumes that the sequence of observations and the associated classifications are dependent. Furthermore, the Markov property states that the classification, which encompasses the prediction of a state, is only dependent on the former classification i.e. the previous instance. 

Let $S_t \in \{1,\ldots, m\}, t \in \mathbb{N}$ be a random variable which determines a state at time step t. Also let $O_t \in \mathbb{R}$ be a random variable of observed data at time step t, which will also be referred to as emissions. Further, define the sequences $\mathbf{O^{(t)}}=(O_1,\ldots,O_t)$ and $\mathbf{S^{(t)}}=(S_1,\ldots,S_t)$. Then, $S_t$ is assumed to follow a first order Markov chain, where for all $t \in \mathbb{N}$ the future state at $t + 1$ solely depends on the current state, fulfilling the Markov property

\begin{equation}
    Pr(S_t | \mathbf{S^{(t-1)}}) = Pr(S_t | S_{t-1}),
    \quad t=2,\ldots,T
    \label{eq:model_markov_property}
\end{equation} 

As such, the Markov property provide a neat mathematical simplification, since if one were to determine the probability that the current state $S_t$ is equal to some state $s_i$ from a full probabilistic system, it would require knowledge of all the states that have come before $S_t$. Given the Markov property, this knowledge can be reduced to only knowing the current state $S_t$. 

The switching behaviour of $S_t$ is governed by the conditional probabilities $Pr(S_{t} = j| S_t = i) = \alpha_{ij}$, where $\alpha_{ij}$ is the probability of switching from state $i$ to state $j$ (Bulla et al., 2011). These are referred to as transition probabilities and they are stored in a transition probability matrix denoted as $\mathbf{A} = \{{\alpha_{ij}}\}$. The initial distribution of the transition probability matrix is stored under the variable $P(S_1|O^{(T)})=\boldsymbol{\delta}$. Furthermore, HMMs operate under the assumption of output independence, which can be expressed through equation \ref{eq: Output independence}.
\begin{equation}
    Pr(O_t|\mathbf{O^{(t-1)}}, \mathbf{S}^{(t)}) = Pr(O_t|S_t),
    \quad t=2,\ldots,T
    \label{eq: Output independence}
\end{equation}

As a result of the output independence, the distribution of the observable data depends on the current state, thereby making the autocorrelation functions highly dependent on the persistence of $O_t$. In addition, we define emission probabilities as $\mathbf{B}=\{ b_j(O_t) \}=Pr(O_t|S_t=j)$, i.e. the probability of observing $O_t$ at time t in state $j$. Since we in this study only consider continuous distributions, $b_j(O_t)$ will represent conditional densities (Jurafsky Martin, 2019).

An HMM is a state-space model with finite state space where equation \ref{eq:model_markov_property} is the state equation and equation \ref{eq: Output independence} is the observation equation. A specific observation can potentially arise from more than one state, since the conditional distributions between the states can overlap.

\subsection{A 2-state Hidden Markov Model}

In the following section, a 2-state HMM is introduced, however, the example could easily be expanded to include more states. Consider the two-state model with Gaussian conditional distributions specifications:
\begin{equation}
     O_t|S_t \sim N(\mu_{S_t},\sigma^2_{S_t}) 
\end{equation}
\begin{equation}
   O_t = \mu_{S_t}  + \sigma_{S_t}\epsilon_{t}
   ,\quad \epsilon_{t} \sim N(0,1)   
\end{equation}
where,

$$
    \mu_{S_t}=
    \begin{cases}
        \mu_1, & \text{if}\ S_t = 1 \\
        \mu_2, & \text{if}\ S_t = 2
    \end{cases},
    \sigma_{S_t} =
    \begin{cases}
        \sigma_1, & \text{if}\ S_t = 1 \\
        \sigma_2, & \text{if}\ S_t = 2
    \end{cases},
    \mathbf{A} = 
    \begin{bmatrix}
    \alpha_{11} & \alpha_{12} \\
    \alpha_{21} & \alpha_{22}
    \end{bmatrix},
    \Theta = (\mathbf{A}, \mathbf{B}, \boldsymbol{\delta})
$$

Where $\Theta$ captures the estimated model parameters. The nature of the workings of a HMM are quite intuitive to comprehend. For every period t, the model observes a realization $o_t$ from $O_t$. As such one can think of this as drawing an observation form an underlying distribution. However, it is unknown whether $S_t = 1$ or $2$. Further, the properties underlying the transition probabilities in $\mathbf{A}$ are also unknown. As such, the objective of the HMM is to estimate the most likely sequence of $S_t$ as well as the most likely parameters, $\mathbf{A}$ \& $B$. 

Furthermore, as previously mentioned, the sojourn times are implicitly assumed to be geometrically distributed as per equation \ref{eq: sojourn time}
\begin{equation}
    Pr('\text{staying t time steps in state i'}) = \alpha^{t-1}_{ii} *(1-\alpha_{ii})
    \label{eq: sojourn time}
\end{equation}
As such, the expected duration of state \textit{i} can be depicted as 
\begin{equation}
    E[r_i] = \frac{1}{1-\alpha_{ii}}
    \label{eq: geometric distribution memory}
\end{equation}

As evident by equation \ref{eq: sojourn time} and \ref{eq: geometric distribution memory}, the geometric distribution of the sojourn times is memoryless, implying that the time until one observes a transition out of the current state is independent of the amount of time spend in the current state. One of the challenging aspect associated with HMMs is estimating the model parameters. This is discussed in the following section.

\section{Estimation}

\subsection{Maximum likelihood estimation}
The traditional way of estimating the parameters making up the HMM is through the Maximum Likelihood
(ML) method. Following this method brings about three problems:

    1. Determine the likelihood of which the model generated a sequence of  observations $\mathbf{O}^{(T)}$. 
    
    2. Estimate the model parameters that maximize the likelihood of the observed data.

    3. Based on the observation sequence, infer the most likely sequence of states $S^{(T)}$
\label{subsection: MLE}
 
In order to solve the first problem, one need to rely on a dynamic programming technique, known as the forward–
backward algorithm. The second problem is solved by relying on the work by Baum et al. 1970 and Dempster et al. (1977), which involves utilizing the Baum–Welch
algorithm. Finally, the third problem is solved by using the Viterbi algorithm. All the algorithms and their respective mathematics are outlined in the next three subsections. 
 
\subsubsection{The Forward-Backward Algorithm}
\label{Section: Forward backward}
In order to motivate the use of the forward-backward algorithm, one can try to imagine all possible combinations of state sequences i.e. $S_t = s_1, s_2, s_3, s_4....s_t$. The state sequence will always have the same length as the observation sequence $O_t$. The relevant information to uncover is the probability of seeing a particular observation sequence, given a state sequence. After uncovering all the possible state sequences that can produce a particular observations sequence, the probabilities can be summed, and the answer simplifies to a likelihood. Mathematically, the likelihood ($L_T$) of the observations given the model parameters $\Theta$, can be written as expressed in equation \ref{eq: motive fb}.

\begin{equation}
    L_T = P(O_t|\Theta) = \sum_{S_1,\ldots,S_t} \delta_{s_1}*b_{s_1}*(o_1) * \alpha_{s_1s_2} * b_{s_2}(o_2)*\alpha_{s_2s_3} .....\alpha_{s_t-1 s_t}*b_{s_t}*(o_t)
    \label{eq: motive fb}
\end{equation}

As expressed through equation \ref{eq: motive fb}, if an observations sequence is assumed to consist of 10 observations, which can be generated by a variety of different state sequences, one must derive the individual probability of each potential state sequence that can generate the correct observation sequence and sum them up to find the probability of  an observation sequence $O_t$ given a model $\Theta$. The problem with this approach is that when the number of states ($m$) and the number of observations ($T$) grows large the complexity increases, and this methodology becomes highly computationally expensive. The number of operations required to estimate equation \ref{eq: motive fb} simplifies to:

\begin{equation}
    (2T-1)*m^T+(m^T-1)
\end{equation}

As such, for an observations sequence with $T = 100$ and $N = 5$, following the procedure of equation \ref{eq: motive fb} would result in $10^{72}$ operations. When the number of observations and states grows larger, this becomes even more computationally heavy, hence a better methodology is needed.  
In order to reduce the computational complexity and solve the problem of deriving the probability of a particular observation sequence, the forward–backward algorithm is used. These results will also be used in the subsequent Baum-Welch algorithm. The ith element of the vector $\mathbf{\alpha_t}$ is the forward probability.

\begin{equation}
    \alpha_t(i) = P(o_1,\ldots,o_t, S_t = i | \Theta)
    \label{eq: forward prob}
\end{equation}

As such, it is evident from equation \ref{eq: forward prob} that the forward trellis can be defined as the probability of seeing observation $o_1 $ to $o_t$ while ending up at state $i$ at time $t$. Therefore, the forward probability $\alpha_t(i)$ will be derived for each possible state at each time $t$. This means that the derivation of the forward probability can be separated in a base, inductive and termination step as depicted by equation \ref{eq: base case forward prob}, \ref{eq: inductive step forward prob} and \ref{eq: termination step}.

\begin{equation}
    \alpha_1(i) = \delta_i*b_i(o_1),
    \quad  1 \leq i \leq m
    \label{eq: base case forward prob}
\end{equation}
\begin{equation}
    \alpha_{t+1}(j) = \Big[\sum_{i=1}^m a_t(i)*a_{ij}\Big] * b_j(o_{t+1}),
    \quad 1\leq t \leq T,
     \quad 1 \leq j \leq m
    \label{eq: inductive step forward prob}
\end{equation}
\begin{equation}
    L_T = P(O^{(T)}|\Theta) = \sum_{i=1}^m a_T(i)
    \label{eq: termination step}
\end{equation}

The base case represents the probability of starting in state $i$ while observing $o_1$. After this initial calculation, which is done for all states, the inductive step is initialized. The inductive step derives the joint probability of seeing state $j$ at $t+1$ and observing the sequence $\mathbf{O}^{(t)}$. As such, the forward probabilities derived one time step before is stored, and each of these stored values are then multiplied with their transition probability of jumping to state $j$ at time $t+1$. Taking the sum of all the possible paths and multiplying with the emission probability of seeing $O_{t+1}$ yields $\alpha_{t+1}(j)$. Therefore, the forward probabilities are inductively rolled forward until termination is reached at $T$. At termination the forward probabilities are summed across all states and a single value is reached which encompass the probability of observing a particular observation sequence based on a given set of model parameters $\Theta$.

The method depicted through equation \ref{eq: base case forward prob}, \ref{eq: inductive step forward prob} and \ref{eq: termination step} provides an intuitive inductive framework as opposed to the more comprehensive methodology introduced by equation \ref{eq: motive fb}. Furthermore, by introducing the forward probability framework, the computational complexity reduces to $O(m^2T)$. This reduction in mathematical operations makes it appropriate to implement the algorithm in an operationally efficient manner, hence the methodology could be applied by institutional investors and fund managers. 

Although the forward probability is all one need to solve the first problem in order to determine the probability of seeing an observation sequence $O_t$ given a model $\Theta$, the same methodology will be conducted backward in order to solve additional problems related to the Baum-Welch algorithm. The backward probability, denoted $\beta_t(i)$ is derived in a similar fashion to the forward algorithm defined in equation \ref{eq: forward prob}.

\begin{equation}
    \beta_t(i) = P(o_{t+1},\ldots,o_T | s_t = i, \Theta) 
\end{equation}

The primary difference between the forward and backward probabilities is that instead of looking at an observation sequence from the past up to $t$, the backward probability determines the probability of seeing a future observation sequence from $t+1$ to $T$ given that the model is in state $i$ at time $t$. As such, the backward probability is reciprocal to the forward probabilities. Similar to the forward probability one can separate the backward probabilities into a base and inductive step. 
\begin{equation}
    \beta_T(i) = 1 % 1\leq i \leq m%
    \label{eq: base backward}
\end{equation}
\begin{equation}
   \beta_t(i) = \sum_{j=1}^m a_{ij}*b_j(o_{t+1})*\beta_{t+1}(j),
   \quad t = T-1,\ldots,1,
   \quad 1\leq i \leq m
    \label{eq: Inductive backward}
\end{equation}

It is evident from equation \ref{eq: base backward} and \ref{eq: Inductive backward} that the backward probabilities are derived by summing over all of the states that the model might end up while taking into account the transition and emission probabilities as well as the $\beta$ at time $t+1$.

From the forward and backward probabilities it follows that the probability of being in state $i$ at time $t$ can be found as

\begin{align}
    Pr(o_1,\ldots,o_t, S_t = i | \Theta) * 
    Pr(o_{t+1},\ldots,o_T | s_t = i, \Theta)
     &=\alpha_t(i) * \beta_t(i) \\
     &= Pr(O^{(T)} = o^{(T)}, S_t = i |\Theta)
\end{align}

As such, the likelihood of the observed data $o^{(T)}$ can be evaluated as by marginalizing over all $m$ states:
\begin{equation}
    Pr(O^{(T)} = o^{(T)} | \Theta) = \sum_{i=1}^m \alpha_t(i) * \beta_t(i)
    \label{eq: solution problem 1}
\end{equation}

As such, one can see that equation \ref{eq: termination step} and \ref{eq: solution problem 1} provide identical results when $t=T$. In relation to the implementation forward-backward algorithm, it is necessary to scale the probabilities to avoid numerical
underflow, as described in Zucchini and MacDonald (2009). 

\subsubsection{The Baum-Welch Algorithm}
There are two popular methodologies to maximizing the likelihood which involves the direct nummerical maximization and the Baum-Welch algorithm, which is a unique case of the more general expectation-maximization (EM) algorithm developed by Baum et al. (1970). Of the two, the Baum-Welch algorithm is often preferred due to its larger robustness to initial starting values (Nystrup, 2014). Yet, the starting values are crucial since even though the likelihood is guaranteed to increase or remain the same for each iteration of the EM algorithm, the convergence towards the global maximum might be slow, or worse, it might get stuck at a local maximum. In order to circumvent this, the paper relies on several runs of the EM algorithm with different starting values, thereby making it possible to uncover any significant deviations in the maximum likelihood. 

The Baum–Welch algorithm maximizes the logarithm of the complete-data likelihood (CDLL), which includes the log-likelihood of the observations $o^{(T)}$ and the hidden data $s^{(T)}$. This can be written as

\begin{equation}
    \log(Pr(o^{(T)},s^{(T)}) = \log(\delta_{s_1} * \Pi_{t=2}^T \alpha_{s_{t-1},s_t} * \Pi_{t=1}^T b_{s_t}(o_t)) 
\end{equation}
This can also be expressed as
\begin{equation}
    \log(Pr(o^{(T)},s^{(T)}) = \log(\delta_{s_1}) + \sum_{t=2}^T \log(\alpha_{s_{t-1},s_t})+\sum_{t=1}^T log(b_{st}(o_t)))
\end{equation}

In order to simplify the operational nature of the maximum likelihood function, the following binary random variables are introduced below.
$$
\gamma_i(t) = 1 \ \text{if and only if}\ s_t = i
$$
and
$$
\xi_{ij}(t) = 1 \ \text{if and only if}\ s_{t-1}=i\ \text{and}\ s_t = j
$$
As such if no state transition occurs $\gamma_i(t)$ is activated and if a state transition occurs $\xi_{ij}(t)$ is activated. The CDLL can now be written as

\begin{equation}
    \log(Pr(o^{(T)}, s^{(T)})) = \sum_{i=1}^m \gamma_i(1) \log\delta_i + \sum_{i=1}^m \sum_{j=1}^m \Big(\sum_{t=2}^T \xi_{ij}(t)\Big) \log\alpha_{ij}+ \sum_{i=1}^m \sum_{t=1}^T \gamma_i(t) \log b_i(o_t) 
    \label{eq: CDLL}
\end{equation}

Now, the idea underlying the methodology of the EM algorithm is to replace the quantities of $\gamma_i(t)$ and $\xi_{ij}(t)$ by their conditional expectations given an observation sequence $o^{T}$ as well as the current model parameters $\Theta(A, B,\delta)$. This is referred to as the estimation-step (E-step) (Nystrup, 2014). 

\begin{equation}
    \hat{\gamma_i(t)} = Pr(s_t=i | o^{(T)}) = \frac{\alpha_i(t)*\beta_i(t)}{\sum_{j=1}^m \alpha_t(j)\beta_t(j)} = \frac{\alpha_i(t)*\beta_i(t)}{P(O_T|\Theta)}
    \label{eq: gamma}
\end{equation}
and
\begin{equation}
    \hat{\xi_{ij}(t)} = Pr(s_{t-1}=i, s_t=j|o^{(T)}) = \frac{\alpha_{t-1}(i)*\alpha_{ij}b_j(o_{t+1}\beta_{t+1}(j)}{\sum_{i=1}^m\sum_{j=1}^m\alpha_{t-1}(i)*\alpha_{ij}b_j(o_{t+1}\beta_{t+1}(j)} = \frac{\alpha_{t-1}(i)*\alpha_{ij}b_j(o_{t+1}\beta_{t+1}(j)}{P(O_T|\Theta)}
    \label{eq: xi}
\end{equation}

As such, it is evident $\gamma_i(t)$ encompass the probability of being in state $i$ at time $t$ knowing all the observations that has come before and all the observations that is to come, regardless of all the possible ways the model could have arrived at $t$ as well as all the possible ways the model can continue after $t$. Given this analogy one can think of $\gamma_i(t)$ as a bowtie that ties the forward and backward probabilities together for a given state $i$ at a given time $t$. As such, given the properties in equation \ref{eq: gamma}, it is evident that $\sum_{i=1}^m \gamma_i(t) = 1$

Furthermore, $\xi_{ij}(t)$ captures the probability of being in state $i$ at time $t$ and then transitioning to state $j$ at time $t+1$, given all the past observations and all the observations to come. Therefore, the similarity to $\gamma_i(t)$ is evident, however, $\xi_{ij}(t)$ takes into account two time steps. As such, given the properties of equation \ref{eq: gamma} and \ref{eq: xi} it is clear that $\sum_{j=1}^m \xi_{ij}(t) = \gamma_i(t)$. 

Having replaced $\gamma_i(t)$ and $\xi_{ij}(t)$ by their conditional expectations $\hat{\gamma_i(t)}$ and $\hat{\xi_{ij}(t)}$ the CDLL from equation \ref{eq: CDLL} is maximized with respect to three sets of parameters which includes the initial distribution $\delta$, the transition probability matrix $A$ as well as the parameters of the state dependent distributions captured by $B$. This process is referred to as the maximization step (M-step). 

As such, the mechanism of the EM algorithm is to repeat the E-step and M-step until one or several convergence criterion have been satisfied. This criterion could entail that the iteration proceeds until the resulting change in the log-likelihood is below some threshold. As briefly mentioned, since the EM algorithm is sensitive to starting values there is no guarantee that the model parameters converge to a global maximum. In order to avoid the model getting stuck in a local maximum or a saddle point, several starting values are tested. 

The solution set to the parameters is 

\begin{equation}
    \delta_i = \frac{\hat{\gamma_i(1)}}{\sum_{i=1}^m\hat{\gamma_i(1)}} = \hat{\gamma_i(1)}
\end{equation}

and

\begin{equation}
    \alpha_{ij} = \frac{\sum_{t=1}^{T-1}\hat{\xi_{ij}(t)}}{\sum_{t=1}^{T-1}\hat{\gamma_i(t)}}
\end{equation}

where $\sum_{t=1}^{T-1}\hat{\xi_{ij}(t)}$ is the total number of expected transitions from state $i$ to state $j$ and $\sum_{t=1}^{T-1}\hat{\gamma_i(t)}$ is the total number of transitions from state $i$.

Furthermore, maximization of the third term in equation \ref{eq: CDLL} may be easy or difficult depending on the nature of the state-dependent distributions (Nystrup, 2014). In some cases numerical maximization will be necessary, but in the case of conditional univariate normal
distributions the maximizing values of the state-dependent parameters simplifies to
\begin{equation}
    b_j(k) = \frac{\sum_{t=1, s.t.o_t}^T\hat{\gamma_i(t)}}{\sum_{t=1}^T\hat{\gamma_i(t)}}    
\end{equation}
where $\sum_{t=1, s.t.o_t}^T\hat{\gamma_i(t)}$ is the number of times in state $j$ observing $o_t$ and $\sum_{t=1}^T\hat{\gamma_i(t)}$ is the number of times in state $j$

In order to utilize this on a large scale, one must carefully consider the speed of convergence. One way to increase the speed of convergence is through a hybrid algorithm that starts with the EM algorithm and switches to a Newton-type algorithm when a certain stopping criterion is fulfilled (Bulla \& Berzel, 2008)

It should be clear that the likelihood of an HMM is a complicated function of defined model parameters hence it has several local maxima. The goal of the algorithm is obviously to find the model parameters that maximizes the likelihood, however, there is no simple way to determine that a global maximum has been reached. As such, the paper utilizes a sensible strategy which involves using a range of starting values for the initial model parameters.

As such, the EM algorithm is circular since the algorithm initializes with a qualified estimate of the model parameters $\Theta(A,\delta, B)$. The forward-backward algorithm then derives $\alpha_t(i), \beta_t(i), \gamma_i(t)$ and $\xi_{ij}(t)$. This is the E-step. These derived parameters are then used to update the model parameters by maximizaing the CDLL log-likelihood function. This completes one iteration of the EM algorithm and the model starts over until it converges to a local or preferably global maximum. Alternative methodologies for maximizing the likelihood function do exist. For instance a Bayesian approach based on Markov chain Monte Carlo (MCMC) sampling can be used (Frühwirth-Schnatter 2006). It follows that Rydén (2008) compared the EM and MCMC approaches and found that MCMC can be advantageous for interval estimation and inferential problems, yet the EM serves as a simpler and faster way of obtaining point estimates.

As such, the forward-backward and Baum-Welch (EM) algorithm have solved the first two problems, hence the only aspect left is to infer the most likely sequence of states. This is done through the Viterbi algorithm. 
 
\subsubsection{Decoding: The Viterbi Algorithm}
\label{subsection: Decoding}
Inferring the most likely sequence of states is referred to as decoding. This is a crucial aspect of the HMM since a particular observation sequence can produce a variety of associated state sequences since several conditional distribution can produce the same observations. Furthermore, decoding becomes important since the parameters must be optimized not just for the most probable sequence of states but the sequence of states must also be viable. For instance if the model suggests a jump from state $i$ to state $j$ at time $t$ to $t+1$, but due to constraints, such as a transition probability matrix of 0\%, this state transition may not viable in reality, hence the model must select another appropriate transition. 

As such, the model should estimate the most probable state sequence while taking into account that the particular state sequence is viable in a real-world context. In addition the decoding is essentially the information that makes it possible to construct a trading / investment strategy based on a HMM, since an investor need to know the states of the economy to allocate capital in an efficient manner. This means that a HMM without a decoding procedure becomes rather useless from an investment perspective. 

Decoding can be done either locally, by determining the most likely state at each time $t$ or globally by determining the most likely sequence of states (Nystrup, 2014). The issue with local decoding is that it deduces the state sequence at each time instead of doing so across the entire time horizon. This can lead to suggested state sequences that are impossible to take, hence global decoding is preferred. 

\begin{equation}
    arg\max_{s^{(t)}}Pr(S^{(T)} = s^{(T)}|O^{(T)} = o^{(T)}, \Theta)
    \label{eq: global decoding}
\end{equation}

Maximizing equation \ref{eq: global decoding} over all possible state sequences by brute force involves mT operations and thus proves inefficient on large scale evaluations. Rather, the Viterbi algorithm can be used to compute
the most likely sequence of states in an efficient manner. 

The best way to comprehend the Viterbi algorithm is to realize that at $t+1$ the algorithm has to figure out the state at time $t$ that maximizes the probability of making a transition to a given state $i$ and observing a specific observation at time $t+1$. Once this is done, the state from time $t$ is stored. Now the algorithm moves on to time $t+2$ and find the state that maximizes a move from time $t+1$ to $t+2$ while observing a specific observation at time $t+2$. Then the state at t+1 that maximizes this is stored and so forth. Once the algorithm terminates at $T$ it will contain a map of valid state sequences that can be followed and interpreted. 

In order to implement the outlined reasoning some additional variables are introduced.

\begin{equation}
    \pi_t(i) =  arg\max_{s^{(t)}}Pr(S^{(T)} = s^{(T)}|O^{(T)} = o^{(T)}, \Theta)
\end{equation}
and 
$$
{\psi_t(i)}
$$

$\pi_t(i)$ finds the path with the highest probability that accounts for the first $t$ observations and ends up at state $i$ at time $t$ and $\psi_t(i)$ serves as a storing variable that keeps track of optimal state at each time step $t$. Similar to the previously described algorithms, the Viterbi algorithm can be separated into an base, inductive and termination step.
\begin{equation}
    \text{Base case: } \pi_1(i) = \delta_i*b_i(o_1)
    \label{eq: viterbi start 1}
\end{equation}
\begin{equation}
     \text{Base case: } \psi_1(i) = 0 
\end{equation}
\begin{equation}
    \text{Inductive step: } \pi_t(j) = \max_{1\leq i \leq m}\Big[\pi_{t-1}(i)a_{ij}\Big] *b_j(o_t), 2\leq t \leq T 
    \label{eq: inductive viterbi}
\end{equation}
\begin{equation}
    \text{Inductive step: }\psi_t(j) = {arg\max_{1\leq i \leq m} \Big[\pi_{t-1}(i)a_{ij}\Big]}, {1\leq j \leq m}
    \label{eq: inductive viterbi 2}
\end{equation}

Naturally, the best initial state is the one that maximizes the probability of observing a particular observation $o_1$ as expressed in equation \ref{eq: viterbi start 1}. Furthermore, it is obvious that the storing variable $psi_t(i)$ is 0 at initialization since no optimal state has been discovered.

The first term in equation \ref{eq: inductive viterbi} is the state that maximized $\pi$ at the previous time step. In this case thate state is set to $i$ but the algorithm scan over all the potential states that the model could have come from. After doing so the algorithm considers all the ways of getting from state  $i$ to $j$ which is the transition probability, which is then multiplied with the probability of observing $ot+1$ at the state $j$. 

Therefore, the max function of equation \ref{eq: inductive viterbi} is going to find the state from the previous time period that maximizes the probability of moving to the current state. Note that the optimal state from the previous iteration is stored in the variable $\psi$ such that the algorithm does not loose track of previous iterations. 

The termination step of the algorithm is presented in equation \ref{eq: viterbi termination 1} and \ref{eq: viterbi termination 2}
\begin{equation}
    \text{Termination: } \pi^* = \max_{1\leq i\leq m}\Big[\delta_T(i)\Big] 
    \label{eq: viterbi termination 1}
\end{equation}
 \begin{equation}
     \text{Termination: } s_T^*= arg\max_{1\leq i\leq m}\Big[\delta_T(i)\Big]
     \label{eq: viterbi termination 2}
 \end{equation}
Where $\pi^*$ expresses the highest probability for the valid sequence of states given an observation sequence. As such, knowing $\pi^*$ it is possible to iterate backwards through the process as
\begin{equation}
    s_t^* = \psi_{t+1}*(s_{t+1}^*)
\end{equation}

Comparing equation \ref{eq: inductive viterbi} with equation \ref{eq: inductive step forward prob} it is evident that the dynamic programming technique for the Viterbi algorithm is the same as in the forward–backward
algorithm. The only difference is the substitution of the sum with a maximization. From a computational efficiency perspective the Viterbi algorithm remains at $O_T*(m^2*T)$, thereby making it feasible for full-scale production applications (Dittmer 2008).

\subsection{Logarithmic scaling to handle numerical underflow}
The theory above have outlined the mathematics comprising HMMs, however, a crucial aspect regarding their implementation is dealing with the underflow and/or overflow produced by the algorithms. The issues regarding underflow arises when $T$ gets since the algorithms will begin multiplying together smaller and smaller numbers when for instance deriving the forward and backward probabilities. Overflow is the inverse to underflow and it occurs when continuously larger numbers are multiplied together. As such, underflow and overflow proposes a challenge to the implementation of the HMMs as Python or other libaries will generate an error or simply interrupt the execution of the code. The challenges associated with underflow and overflow is an area that is widely overlooked in the literature, despite the fact it is of crucial importance in terms of utilizing HMMs on a full-scale operational level. This section aims at proposing a solution towards the challenges associated with underflow and overflow.

\subsubsection{Scaling the likelihood computation}
For discrete state-dependent distributions, the element of the forward probabilities $\alpha_t$ ,as seen in equation \ref{eq: inductive step forward prob}, become progressively smaller as $t$ increase because the forward probabilities are made up of products of probabilities, hence these will eventually be rounded to 0. The remedy needed to circumvent this issue is the same for overflow and underflow, hence the authors confine their attention to underflow.

A circumvention of the challenges related to underflow can be implemented by computing the logarithm of $L_T$ in equation \ref{eq: motive fb} which involves using a strategy of scaling the vector of forward and backward probabilities $\alpha_t$ and $\beta_t$. As such, a scaling of the vector of forward and backward probabilities can be conducted by log-transforming the  traditional formulas presented in section \ref{Section: Forward backward}. This methodology does not change the workings of the algorithms, however, it reduces the problem associated with underflow as the forward and backward probabilities no longer becomes excessively small when $t$ increases. Since the log-transformation will be rooted in the formulas presented in \ref{Section: Forward backward}, one can separate the transformations into a base, inductive and termination step.  
\begin{equation}
    \text{Log Base case}: \log\alpha_1(i) = \log\delta_i + \log b_i(o_1),  1 \leq i \leq m
    \label{eq: log base case forward prob}
\end{equation}

\begin{equation}
    \log\alpha_{t+1}(j) = \log\Big[\sum_{i=1}^m \exp^{\log a_t(i)+\log a_{ij}}\Big] + \log b_j(o_{t+1}), 1\leq t \leq T - 1,
    1 \leq j \leq m
    \label{eq: log inductive step forward prob}
\end{equation}

\begin{equation}
     \text{Termination step}: \log P(O^{(T)} = o^{(T)}|\Theta) = \log\sum_{i=1}^m \exp^{\log\alpha_T(i)}
    \label{eq: log termination step}
\end{equation}

As should be evident by equation \ref{eq: log base case forward prob}, \ref{eq: log inductive step forward prob} and \ref{eq: log termination step} a large part of the transformation relies on the following mathematics $log(ab) = log(a) + log(b)$. Furthermore, the transformation, particularly the transformation in the inductive step from equation \ref{eq: log inductive step forward prob}, relies on the mathematics guiding the LogSumExp (LSE) function. The LSE is defined as $LSE(x_1,...,x_n) = log(exp^{(x_1)}+...+exp^{(x_n)})$. One of the most common purposes of relying on log-transformations for algorithmic computations is to increase accuracy and avoid underflow. 

Yet, the utilization of the LSE function directly, as depicted in equation \ref{eq: log inductive step forward prob}, can result in additional problems related to overflow/underflow. Therefore, the following formula, which provides equivalent results as the provided LSE formula, must be used instead. The revised LSE formula is defined as $LSE(x_1,....,x_n) = x^* + log(exp(x_1-x^*)+....+exp(x_n-x^*))$ where $x^* = max(x_1,.....,x_n)$. Despite having more than one variable, the procedure remains the same, yet the $x^*$ will be selected from range of multiple vectors. This mathematical trick is done in order to ensure that the number of which the logarithm is taken is above 0, since the natural logarithm is undefined for values below 0. 

By utilising the logarithmic transformation formulas just presented, it is possible to conduct a similar transformation for the backward probabilities derived from equation \ref{eq: base backward} and \ref{eq: Inductive backward}. 

\begin{equation}
     \textit{Base case}: \log\beta_T(i) = 0
    \label{eq: log base backward}
\end{equation}

\begin{equation}
    \textit{Inductive step}: \beta_t(i) = \log\sum_{j=1}^m \exp^{\log a_{ij}+\log b_j(o_{t+1})+\log \beta_{t+1}(j)}
    \label{eq: log inductive backward}
\end{equation}


\subsection{Jump estimation}
\label{subsection: Jump theory}

Apart from maximum likelihood estimation, other ways to estimate HMM's include jump estimation as shown by Bemporad et al. (2018). When given data $o_t\in O$, $t=1,\ldots,T$, then for $O = \mathbb{R}^d$, $o$ is a feature vector, which can be both univariate and multivariate. We are additionally given model parameters $\theta_s\in \mathbb{R}^d$, $s=1...,K$ and the latent state variable $s_t\in\mathbb{N}$, which determines which model parameter $\theta_{s_t}$ is active at time t. Then the loss function $\ell: O\times\mathbb{R}^d \rightarrow\mathbb{R}$, the regularizer term $r: \mathbb{R}^d \rightarrow \mathbb{R}$ and the state sequence loss $\mathcal{L}$ defines the generalized jump model fitting objective
\begin{equation}
    J(O, \Theta, S) = \sum_{t=1}^T \ell(o_t,\theta_{s_t}) + \sum_{k=1}^K r(\theta_{s_t}) + \mathcal{L(S)}
\label{eq:jump_gen_objective}    
\end{equation}
which is minimized with respect to $\Theta=(\theta_1,\ldots,\theta_K)$ and the latent state sequence $\mathcal{S}=(s_1,\ldots,s_T)$. Thus, the general idea is that based on data O, we choose a number of K states and then minimize $J(O, \Theta, S)$ after which the distributional properties of each state $s$ will be fully explained by the model. Viewing the elements of $\Theta$ as cluster centers, the similarities to K-models models such as the K-means are clear. Bemporad et al. (2018) showed how HMM's can be seen as a special case of \cref{eq:jump_gen_objective} and for $\mathcal{L(S)} = \lambda\mathbf{1}_{\{ s_t\ne s_{t+1}\}}$, $\lambda \in \mathbb{R}$, and $r(\theta_{s_t})=0$, Nystrup et al. (2020) proposed the following alteration of \cref{eq:jump_gen_objective} for fitting fitting HMM's
\begin{equation}
    \sum_{t=1}^{T-1}[\ell(o_t, \theta_{s_t}) + \lambda\mathbf{1}_{\{ s_t\ne s_{t+1} \}}]
    + \ell(o_T, \theta_{s_T})
    ,\quad \lambda \geq 0
\label{eq:jump_objective}
\end{equation}

In \cref{eq:jump_objective}, $\lambda$ is a penalty that applies each time a state switch occur. Note that for $\lambda=0$, \cref{eq:jump_objective} reduces to a generalized K-means model (Lloyd, 1982), as the time ordering of observations become irrelevant. For $\lambda$ large enough, the model will converge towards a single state which will cover the entire dataset, thus $\lambda$ can be used to tune the persistence of states. The same loss as used by Nystrup et al. (2020) will be used in this analysis, making the loss function equal to the squared L2 norm\footnote{The squared L2 norm is defined as $\|\mathbf{x}\|_2^2 = (\sqrt{\sum_i x_i^2})^2 = \sum_i x_i^2 = \mathbf{x'x} $} $\ell(o_t, \theta_{s_t}) = \| o_t - \theta_{s_t} \|_2^2$, which is equivalent to the loss in K-means clustering.

\cref{algo:jump_fit} describes our estimation approach to fitting jump models. From a univariate time series $O$, a number of standardized features $Z$ are constructed and inputted to the algorithm along with the number of latent states K and an initial guess of the state sequence $\mathcal{S}$. Then, the algorithm iterates between fitting the model that minimizes the loss function for a given state sequence, and finding the state sequence that minimize the objective given in \cref{eq:jump_objective}. The algorithm is terminated once the state sequence doesn't change from one iteration to the next, or when the change in the objective function is below some tolerance level. Upon termination, transition probabilities are found by counting the number of state transitions and sojourns from the final state sequence $\mathcal{S}^i$. The distributional properties of the HMM is estimated from the sample mean and sample variance in each state.

\begin{algorithm}[H]
\DontPrintSemicolon

\KwInput{Time series O, number of latent states K and initial state sequence $\mathcal{S}^0=(s_0^0,\ldots,s_T^0)$.}
\BlankLine

1. Construct a set of standardized features Z from O.\;
2. Iterate for $i=1,\ldots$ until $S^i=S^{i-1}$ \;
\Indp
(a) $\Theta^i = arg\min_{\Theta} \sum_{t=1}^T \ell(z_t, \theta_{s_t^{i-1}})$ (model fitting)\;
(b) $S^i = arg\min_s \sum_{t=1}^{T-1} [\ell(o_t, \theta_{s_t}) + \lambda\mathbf{1}_{\{ s_t\ne s_{t+1} \}}]
    + \ell(o_T, \theta_{s_T})$ (state sequence fitting)\;
\Indm
3. Compute transition probabilities and distributional properties in each state.\;
\BlankLine
\KwOutput{HMM parameters and prediction of latent states.}

\caption{Jump estimation of HMM (Nystrup et al., 2020)}
\label{algo:jump_fit}
\end{algorithm}

The objective function in \cref{eq:jump_objective} is guaranteed to stay the same or decrease for every iteration in \cref{algo:jump_fit}, yet just as the EM algorithm from previous sections, \cref{algo:jump_fit} is sensitive towards initial starting values and may converge towards local minima. As a result, we run \cref{algo:jump_fit} from 10 different starting values generated using K-means++ (Arthur \& Vassilvitskii, 2007) the same way as proposed by Nystrup et al. (2020).

In part 1 of \cref{algo:jump_fit}, we construct a number of standardized features from the observed data $O$. The point of this exercise is that the inclusion of time series features might reveal additional information which we can use in making the model better respond to state switches. Zheng et al. (2019) originally introduced a feature set, with the purpose of better characterising their conditional distributions in a spectral clustering estimator for HMM's. We will build on these features, however, we will exclude all forward looking variables. Using windows length of of $l=6$ and $l=14$, the features are summarized in \cref{algo:jump_features}.

\begin{algorithm}[H]
\DontPrintSemicolon

\KwInput{Time series O, number of latent states K and initial state sequence $\mathcal{S}^0=(s_0^0,\ldots,s_T^0)$.}
\BlankLine

1. Construct a set of standardized features Z from O.\;
2. Iterate for $i=1,\ldots$ until $S^i=S^{i-1}$ \;
\Indp
(a) $\Theta^i = arg\min_{\Theta} \sum_{t=1}^T \ell(z_t, \theta_{s_t^{i-1}})$ (model fitting)\;
(b) $S^i = arg\min_s \sum_{t=1}^{T-1} [\ell(o_t, \theta_{s_t}) + \lambda\mathbf{1}_{\{ s_t\ne s_{t+1} \}}]
    + \ell(o_T, \theta_{s_T})$ (state sequence fitting)\;
\Indm
3. Compute transition probabilities and distributional properties in each state.\;
\BlankLine
\KwOutput{HMM parameters and prediction of latent states.}

\caption{Jump estimation of HMM (Nystrup et al., 2020)}
\label{algo:jump_features}
\end{algorithm}


Part 2.a in \cref{algo:jump_fit} can be easily minimized analytically when the loss has strictly convex forms such as the squared L2 norm used here, thus guaranteeing the solution to be the global minimum. Taking the partial derivative of $\sum_{t=1}^T \ell(z_t, \theta_{s_t^{i-1}})$ with respect to $\theta_j$ yields
\begin{equation}
   \frac{\delta \sum_{t=1}^{T} ((z_t-\theta_{s_t})'(z_t-\theta_{s_t}) }{\delta \theta_j}
   = \frac{\delta \sum_{t=1}^T (z_t'z_t - 2z_t'\theta_{st}+ \theta_{s_t}'\theta_{s_t} )}{\delta  \theta_j}
   = \sum_{t:s_t=j} ( 2\theta_{s_t}' - 2z_t' )
\label{eq:jump_derivative}
\end{equation}
Thus, the partial derivative for $\theta_j$ is the sum over all time steps t where the state $s_t=j$ of the matrix $(\theta_{st}'-2z_t')$. Setting \cref{eq:jump_derivative} equal to zero and solving for $\theta_j$ yields the solution: $\theta_j = \frac{1}{N_j} \sum_{t:s_t=j} z_t $, where $N_j$ is the number of elements in $\mathcal{S}$ that equals $j$. The solution in each state is thus simply a vector of averages from each times series features in $Z$.

Item 2.b in \cref{algo:jump_fit} is minimized recursively in a similar way to the viterbi algorithm explained eariler, except the time order of operations are reversed. Concretely, we 

\begin{equation}
    state sequence fitting = .....
\end{equation}

One of the main advantages to the jump model over maximum likelihood estimation is that the jump model fitting is entirely based on Euclidean distances and does not assume the states to follow any specific distributions. As such it is expected to be more robust when trained on data in which the underlying distibution(s) is unknown, which is often the case with financial data. Additionally, the model is computationally lighter and often only requires between 5-10 operations to converge whereas the MLE estimator often requires 50-100.



\subsubsection{Input and features of the model}

\subsubsection{Model initialization}