\newpage

\section{Simulation study}
In this section HMMs are trained based on the estimation procedures described throughout \ref{section: estimation}. The primary purpose of the section is to compare how well different estimation methods converge toward the true parameter values when data is simulated from an HMM. The advantage of using simulated data is that it entails knowing the true parameter values. Therefore, it becomes possible to directly analyze whether a model suggests parameter estimates close to the true values, as opposed to relying on general measures of fit such as AIC and BIC scores, which is necessarily the case when one works with real data. More importantly, simulated data allows for comparing the true state sequences with the estimated ones. This can be highly informative as it allows for measuring of the accuracy in state classification. Simulation will be carried out under various alterations designed to test the models' convergence when some of their key assumptions about the data are (partially) unfulfilled\footnote
{Such as the maximum likelihood estimator, which assumes conditional normality. Since it has been shown that e.g. conditional students t-distributions are often a better fit the assumption is in practice often not fulfilled (Bulla et al. 2011).
}.

In the following paragraphs, the accuracy of state predictions is defined using recall/sensitivity $\frac{tp}{tp+fn}$, where $tp$ is the number of true positives, and $fn$ is the number of false negative. It measures the percentage of actual positives that the model is able to detect correctly. When the true state is one, then the positive class will also be one, and the negative class will be all other predictions. When the true state is two the positive class will be two$,\ldots,$ and so on. However, one of the issues with recall is its inability to account for imbalanced data sets. To account for this, Nystrup et al. (2020) proposed using a balanced accuracy score (BAC), which is the average of accuracy in each observed state,
\begin{equation}
    BAC = \frac{1}{K} \sum_{k=1}^K \frac{tp_k}{tp_k + fn_k}
    \label{eq: BAC definition}
\end{equation}
where $tp_k$ is the number of true positives and $fn_k$ is the number of false negatives in state $k$. As a result, when a classifier performs equally well on all states, the BAC reduces to a regular accuracy, but if a classifier only performs well by always predicting the dominating class then BAC appropriately drops to the reciprocal of the number of states. Therefore, given data with two classes, if 99\% of the data belongs to the positive class and this is correctly detected, but the remaining 1\%, which belongs to the negative, is incorrectly classified, then the overall classifier obtains a BAC of 0.50, whereas regular accuracy would be 0.99. Considering the previously mentioned gain/loss asymmetry in financial returns where losses occur less frequently, yet their movements are larger than the corresponding movements in gains on average, BAC behaves exactly as desired.

An inherent problem with comparing several HMMs is the fact that they output label-less states (predictions), which is due to their nature being unsupervised machine learning models. This is not only the case in this simulation study, in which thousands of models are compared, but also when back-testing an HMM based on rolling windows on financial data. For example, consider training two HMMs by MLE on the same data with hidden states $a$ and $b$. The differences among the models' final parameter values and state sequences are then entirely attributable to the initialized parameter values (as explained in section \ref{section: estimation}, which are random quantities. If the models are initialized in a way where the first model labels state $a$ as 1 and state $b$ as 2 and the second model have the order reversed, it will make make a direct comparisons of the two, perhaps equally good, models difficult. There are several ways to circumvent this issue, such as choosing the labels that maximize the accuracy score, however, since this is impossible to do on real data another method is used here. Since this thesis relies on the fundamental assumption that the primary difference between conditional Gaussian states is the conditional variance, parameters and state labels are sorted according to variance. Another option could have been sorting the states according to their means, however, when thinking about a "good" and "bad" state it is certainly a possibility to observe a state with a high mean and variance, which would still be interpreted as the "bad" state, thereby making such sorting a poor choice.

The rest of the section is structured as follows. Firstly, the simulation procedure is described. Secondly, a detailed analysis is conducted in order to uncover how to tune the jump penalty $\lambda$ in the jump model. Thirdly, once the penalty has been tuned, the models are trained on simulated data and compared. Initially, the models are trained on correctly specified conditional distributions. This is followed by an analysis that shows the models' performance when conditional distributions are purposely misspecified, since this is a realistic scenario when estimating HMMs on real data.

\subsection{Simulation procedure}

As just described, data is simulated from a 2-state HMM throughout this section, yet in order to make the results comparable to those of Nystrup et al. (2020), the same parameters are chosen, thus yielding the Gaussian HMM.
\begin{equation*}
    O|\S \sim N(\mu_{st}, \sigma_{st}^2)
\end{equation*}
with parameters
$$
    \mu_{s_t}=
    \begin{cases}
        \mu_1= 0.0123 \\
        \mu_2= -0.0157
    \end{cases}, \quad
    \sigma_{s_t} =
    \begin{cases}
        \sigma_1 = 0.0347 \\
        \sigma_2 = 0.0778
    \end{cases}, \quad
    \mathbf{Q} = 
    \begin{bmatrix}
        0.9629 & 0.0371 \\
        0.2101 & 0.7899
    \end{bmatrix}
$$
The model is estimated by Hardy's (2001) using monthly returns, which has been shown to capture prominent stylized facts of financial times series such as volatility clustering and leptokurtosis well. There is a significant overlap between the two states, making it challenging to correctly infer the unobserved state sequence. By assuming instantaneous log returns follow a LÃªvy process, the parameters are transformed from one time scale $t_1$ to another $t_2$ using
\begin{align*}
    \mu_s(t_1) / t_1 &= \mu_s(t_2) / t_2 \\
    \sigma_s(t_1) / \sqrt{\sigma_s(t_1)} &= \sigma_s(t_2) / \sqrt{\sigma_s(t_2)} \\
    \mathbf{Q}(t_1)^{1/t_1} &= \mathbf{Q}(t_2)^{1/t_2}
\end{align*}

where $\mu(t), \sigma(t)$ and $\mathbf{Q}(t)$ are parameters associated with time scale $t$. Assuming there is twenty trading days in a month, i.e. $t_{monthly}=20$ and $t_{daily}=1$, monthly parameters are transformed into daily
$$
    \mu_{s_t}=
    \begin{cases}
        \mu_1= 0.0615 \times 10^{-2} \\
        \mu_2= -0.0785 \times 10^{-2}
    \end{cases}, \quad
    \sigma_{s_t} =
    \begin{cases}
        \sigma_1 = 0.7759 \times 10^{-2} \\
        \sigma_2 = 1.7400 \times 10^{-2}
    \end{cases}, \quad
    \mathbf{Q} = 
    \begin{bmatrix}
        0.9979 & 0.0021 \\
        0.0120 & 0.9880
    \end{bmatrix}
$$

Each series simulated from this model is generated as follows. Firstly, the element $s_0$ is drawn from the stationary distribution $\pi$, and each subsequent element is drawn from $q_{t-1}$, i.e. the row of $Q$ corresponding to the last state sojourn. Based on the value of each $s_t$, observations $o_t$ are generated from the conditional distributions. By repeating this procedure, 1000 different series are simulated with sample lengths $H = 250, 500, 1000, 2000$

\subsection{Hyperparameter tuning in Jump model}
\label{subsection: jump_penalizer}
Since the MLE model does not contain any hyperparameters, it can be applied to the simulated data right away. However, this is not the case for the jump model which requires a given value for $\lambda$. As previously mentioned, the objective is to test the models' ability to correctly identify state sequences, hence the best model is the one to maximize BAC with respect to $\lambda$. Since BAC is a random quantity\footnote
{It is a function of the data and the initial state sequence $s_0$, which in the jump estimator is generated by K-means++. Both quantities are random.},
it is appropriate to define the estimator $\Omega(\lambda)= \mathbb{E}[BAC|\lambda]$, which is maximized with respect to $\lambda$. This can be defined as, 
\begin{equation}
    \hat\Omega(\lambda) = \frac{1}{N} \sum_{i=1}^N BAC_{i, \lambda}^H
\end{equation}
where $N$ is the amount of simulated series, and $BAC_{i, \lambda}^H$ refer to the BAC of the $i$th sequence of sample size $H$ computed using a jump model with penalty $\lambda$.

Having defined these properties the analysis proceed by computing $BAC_{i, \lambda}^H$ on samples lengths 250, 500, 1000 and 2000. Further, penalties are considered on a grid defined on the logarithmic scale with base 2. Initially the grid was defined on a logarithmic scale with base 10, but evidently this was too course to yield meaningful results. Grid points are equidistantly placed on the closed interval log-scale $[2^{-2}, 2^{7}]$. An overview of the plots of the BAC against different levels of the jump penalizer $\lambda$ across all sample lengths for the a variety of feature sets will be provided in section \ref{subsubsection: optimal jump penalizer}.  


\subsubsection{Selecting the optimal jump penalizer and feature set}
\label{subsubsection: optimal jump penalizer}
One of the main challenges when it comes to selecting the jump penalizer is to determine which features to show the model during estimation. As such, an endless amount of features can be constructed from the daily log returns of the S\&P 500 index, however, the challenge is centered around deciding whether to include all the features, or perhaps drop some of them. This type of analysis is desirable to conduct since it makes it possible to compare performance when including and excluding different features. For instance, if a feature's impact on BAC is minimal, it might prevent the model of obtaining any significant results, since the features will be clustered with noise and randomness. As such, selecting the features for estimation of the jump penalizer is somewhat comparable to the classical bias-variance trade-off in machine learning. 

Following this acknowledgement, 3 different feature sets will be constructed and serve as input for the simulation procedure, in which 1000 different series are simulated, with sample lengths $H = 250, 500, 1000, 2000$, as explained in section \ref{subsection: jump_penalizer}. Having completed the simulation, the BAC score will be calculated for each sample length $H$ and the appropriate jump penalizer can be derived. As such, the objective of the analysis is to select the most optimal size of the jump penalizer while achieving the highest BAC. The reader should note that the analysis will not conclude whether the feature space is optimal, since there is no method of doing so. This is because an unlimited amount of features can be derived from the log return series (TansjÃ¶, 2020). 

Using windows length of $l = 6$ and $l = 14$, the first feature set is defined as in algorithm \ref{algo:jump_features_set_1}. 

\begin{algorithm}[H]
\KwInput{Time series O, and window length $l$}
\BlankLine
1. Observation: $o_t$\;
2. Left absolute change: $|o_t-o_{t-1}|$ \;
3. Previous left absolute change: $|o_{t-1}-o_{t-2}|$ \;
4. Left local mean: $mean(o_{t-l},\ldots,o_{t})$ \;
5. Left local std: $std(o_{t-l},\ldots, o_{t})$ \;
6. Left local median: $median(o_{t-l},\ldots,o_{t})$ \;
\BlankLine

\KwOutput{Feature set 1, where each each feature $z_i$ is standardized as $z_i = \frac{\bar x-x_i}{\sigma}$}
\caption{Feature set 1 used in Jump estimation of HMM's}
\label{algo:jump_features_set_1}
\end{algorithm}

The results from the simulation based on the features from algorithm \ref{algo:jump_features_set_1} are shown in figure \ref{fig: BAC plot feature set 1}. It is evident by the figure that there is a substantial difference between the sample lengths and their respective BAC. Overall the BAC score is increasing in the sample length. 

\begin{figure}[H] 
    \centering
    \includegraphics[width=1\textwidth]{analysis/model_convergence/images/jump_penalties_feature_set_1.png}
    \caption{Balanced accuracy of jump estimator as a function of the penalty $\lambda$ using different simulation lengths and the features defined in algorithm \ref{algo:jump_features_set_1}. All points are based on the estimate $\hat{\Omega} (\lambda)$ based on 1000 simulated series. Note that $\lambda$ has been log scaled with base 2.}
    \label{fig: BAC plot feature set 1}
\end{figure}

An interesting point is the varying sensitivity of the jump penalizer $\lambda$ across the different sample lengths. As such, it is evident from figure \ref{fig: BAC plot feature set 1} that the sensitivity of the jump penalizer stabilises at different levels of $\lambda$. For sample lengths of 250 and 500 the BAC stabilises when $\lambda$ is $2^3$ and $2^1$ respectively, however, for sample lengths 1000 and 2000 the BAC never fully stabilises since it is negatively impacted by decreasing levels of $\lambda$ in the range of $2^{-1}$ to $2^4$. This could suggest that the selected features are not strong enough to be used on a stand-alone basis for obtaining a stable jump penalizer for the model. On the other end of the spectrum when $\lambda$ grows large, which for the features defined in algorithm \ref{algo:jump_features_set_1} is when $\lambda > 2^5$, the jump estimator only identifies the dominating state, hence the BAC drops to a level fluctuating between 0.50 to 0.60 depending on the sample length. This is a natural consequence of its definition in equation \ref{eq: BAC definition}. Following this, when $\lambda$ is sufficiently small, BAC converges to the unpenalized case, which is essentially the same as applying a K-means model where the time ordering of observations becomes irrelevant. 

The results showcased by figure \ref{fig: BAC plot feature set 1} indicate that the jump penalizer should be chosen as $\lambda = 2^4$ since this is around the level in which the BAC is maximized for sample lengths 500, 1000 and 2000. Unfortunately, it appears that the maximum BAC that is achievable is around 0.87, which suggests that the state predictions are prone to errors and miss-specifications. As a consequence of this a new feature set will be defined and tested. The feature set will entail a variety of different parameters that are typically found in technical analysis trading strategies. The window length remains identical to that of feature set 1.

\begin{algorithm}[H]
\KwInput{Time series O, and window length $l$}
\BlankLine
1. Min/Max difference: $max(o_{t-l},\ldots,o_{t}) - min(o_{t-l},\ldots,o_{t})$ \;
2. Exponential moving average (EMA) 10 days: $a * o_t + (1-a) * EMA_{t-1}$, \quad where $a$ is the smoothing factor, $a = \frac{2}{10+1}$ \;
3. Upper Bollinger band: $mean(o_{t-l},\ldots,o_t) + 1.96 * std(o_{t-l},\ldots,o_t)$ \;
4. Lower Bollinger band: $mean(o_{t-l},\ldots,o_t) - 1.96 * std(o_{t-l},\ldots,o_t)$\;
5. Bollinger band width: $Upper\ Bollinger\ Band - Lower\  Bollinger\ Band$ \;
6. Bollinger band \%B: $\frac{mean(o_{t-l},\ldots,o_{t}) - Lower\ Bollinger\ band} {Upper\ Bollinger\ band - Lower\ Bollinger\ band}$  \;
\BlankLine
\KwOutput{Feature set 2, where each each feature $z_i$ is standardized as $z_i = \frac{\bar x-x_i}{\sigma}$}
\caption{Feature set 2 used in Jump estimation of HMM's}
\label{algo:jump_features_set_2}
\end{algorithm}

The results based on the features from algorithm \ref{algo:jump_features_set_2} are shown in figure \ref{fig: BAC plot feature set 2}. It is evident from the figure that  there is a substantial difference between the sample lengths and the size of the BAC. This is in line with the results obtained through feature set 1 in algorithm \ref{algo:jump_features_set_1}. Another similarity between the two feature sets is the fact that the BAC is increasing in the sample length.

\begin{figure}[H] 
    \centering
    \includegraphics[width=1\textwidth]{analysis/model_convergence/images/jump_penalties_feature_set_2.png}
    \caption{Balanced accuracy of jump estimator as a function of the penalty $\lambda$ using different simulation lengths and the features defined in algorithm \ref{algo:jump_features_set_2}. All points are based on the estimate $\hat{\Omega} (\lambda)$ based on 1000 simulated series. Note that $\lambda$ has been log scaled with base 2.}
    \label{fig: BAC plot feature set 2}
\end{figure}

Interestingly, it appears that the BAC is less sensitive to changing jump penalizers compared to figure \ref{fig: BAC plot feature set 1}. This is evident since the BAC is a horizontal line, for all sample lengths, when the jump penalizer lies in the range $2^{-1}$ to $2^3$. The horizontal line is a favourable property as it provides a higher confidence in the selected size of the jump penalizer since state prediction results will be consistent across $\lambda$. Furthermore, it is evident that the the BAC have been shifted upwards, for all sample lengths, when compared to the results from figure \ref{fig: BAC plot feature set 1}. This means that the features defined in algorithm \ref{algo:jump_features_set_2} appear to be much stronger in achieving accurate state predictions. 

Finally, it was evident by figure \ref{fig: BAC plot feature set 1} that the different sample lengths appeared to maximize BAC for different levels of $\lambda$, however, when analysing figure \ref{fig: BAC plot feature set 2} it appears that the BAC maximizes at the same $\lambda$ for all sample lengths. This property further provides confidence when selecting the optimal level of $\lambda$, since the result is unanimous across sample lengths. As such, the results showcased by figure \ref{fig: BAC plot feature set 2} indicate that the jump penalizer should be chosen as $\lambda = 2^4$ since this is where BAC is maximized for all sample lengths. This finding is particularly interesting because this is the same size of $\lambda$ that was suggested by using the features defined in algorithm \ref{algo:jump_features_set_1}. Therefore, the conclusion from both feature sets point towards a jump penalizer of the size $2^4$, although the second feature set achieves a much higher BAC across samples and jump penalizers. Due to these findings an appropriate analysis would be to make an identical analysis which include all features as defined in algorithm \ref{algo:jump_features_set_3}. The corresponding grid plot is shown in figure \ref{fig: BAC plot feature set 3}.

\begin{algorithm}[H]
\KwInput{Time series O, and window length $l$}
\BlankLine
1. Observation: $o_t$\;
2. Left absolute change: $|o_t-o_{t-1}|$ \;
3. Previous left absolute change: $|o_{t-1}-o_{t-2}|$ \;
4. Left local mean: $mean(o_{t-l},\ldots,o_{t})$ \;
5. Left local std: $std(o_{t-l},\ldots, o_{t})$ \;
6. Left local median: $median(o_{t-l},\ldots,o_{t})$ \;
7. Min/Max difference: $max(o_{t-l},\ldots,o_{t}) - min(o_{t-l},\ldots,o_{t})$ \;
8. Exponential moving average (EMA) 10 days: $a * o_t + (1-a) * EMA_{t-1}$, \quad where $a$ is the smoothing factor, $a = \frac{2}{10+1}$ \;
9. Upper Bollinger band: $mean(o_{t-l},\ldots,o_t) + 1.96 * std(o_{t-l},\ldots,o_t)$ \;
10. Lower Bollinger band: $mean(o_{t-l},\ldots,o_t) - 1.96 * std(o_{t-l},\ldots,o_t)$\;
11. Bollinger band width: $Upper\ Bollinger\ Band - Lower\  Bollinger\ Band$ \;
12. Bollinger band \%B: $\frac{mean(o_{t-l},\ldots,o_{t}) - Lower\ Bollinger\ band} {Upper\ Bollinger\ band - Lower\ Bollinger\ band}$  \;
\BlankLine
\KwOutput{Feature set 3, where each each feature $z_i$ is standardized as $z_i = \frac{\bar x-x_i}{\sigma}$}
\caption{Feature set 3 used in Jump estimation of HMM's}
\label{algo:jump_features_set_3}
\end{algorithm}


\begin{figure}[H] 
    \centering
    \includegraphics[width=1\textwidth]{analysis/model_convergence/images/jump_penalties_feature_set_3_all.png}
    \caption{Balanced accuracy of jump estimator as a function of the penalty $\lambda$ using different simulation lengths and the features defined in algorithm \ref{algo:jump_features_set_3}. All points are based on the estimate $\hat{\Omega} (\lambda)$ based on 1000 simulated series. Note that $\lambda$ has been log scaled with base 2.}
    \label{fig: BAC plot feature set 3}
\end{figure}

Evidently, several interesting aspects occur when using all the defined features from algorithm \ref{algo:jump_features_set_3} to determine the jump penalizer. Firstly, the plots of the BAC against the jump penalizers becomes stable for a larger domain of definition, since the plots are horizontal for jump penalizers in the range of $2^{-1}$ to $2^4$, which is longer compared to that of figure \ref{fig: BAC plot feature set 2}. Secondly, the BAC is shifted slightly upwards in figure \ref{fig: BAC plot feature set 3} compared to figure \ref{fig: BAC plot feature set 2}. This means that using all the features result in a slightly better BAC compared to solely relying on the features defined in algorithm \ref{algo:jump_features_set_2}. Furthermore, a particular interesting finding is the apparent shift of the optimal jump penalizer. As such, it is evident from figure \ref{fig: BAC plot feature set 3} that the BAC maximizes when $\lambda$ is $2^5$, for all sample lengths. This is contrary to the findings from figure \ref{fig: BAC plot feature set 1} and \ref{fig: BAC plot feature set 2} both pointing towards an optimal jump penalizer of $2^4$. Despite this inconsistency, all 3 feature sets point towards a jump penalizer in the range of $2^4$ to $2^5$, which contribute to a high confidence when selecting the $\lambda$.

In addition, it can be concluded from the analysis of the 3 feature sets, that increasing the sample length increases the BAC. As such, further analysis should consider testing longer sample lengths across different feature sets in order to uncover an optimal combination that pushes  the BAC even higher. This is because it appears that there is a fast decreasing marginal increase in BAC of using additional features, evident by comparing figure \ref{fig: BAC plot feature set 2} and \ref{fig: BAC plot feature set 3}. Conclusively, due to the fact that the BAC is slightly larger and horizontal for a longer domain of definition, for all sample lengths, the jump penalizer will be selected based on the feature set defined in algorithm \ref{algo:jump_features_set_3}, which means that the optimal $\lambda$ is $2^5$. 

\subsection{Simulation study with correctly specified distributions}

\cref{tab:jump_gaussian} and \cref{fig:jump_normal} summarizes the performance of the MLE and Jump estimator for various sample sizes when compared to the true values. As mentioned in the previous section, data is generated from a conditional gaussian HMM, which is also the case for the models trained here. Apart from the HMM parameters, the accuracy in each state as well as BAC is reported. Accuracy for for the true parameters are obtained by running the Viterbi algorithm using an HMM with the true model parameters. This can be seen as the best obtainable performance in state detection and is used to get a sense of where the 'upper bound' on BAC lies when comparing the other models.

\textbf{Vis om estimatoren er konsistent.}

\begin{figure}[H] 
    \centering
    \includegraphics[width=1\textwidth]{analysis/model_convergence/images/simulation_normal.png}
    \caption{Estimates of HMM models' convergence towards true values as a function of simulation length. Results are based on 1000 simulations from conditional gaussian distributions.}
    \label{fig:jump_normal}
\end{figure}

It is clear that that both models converge towards the true values when longer simulation lengths are considered. Curiously, the jump estimator is able to detect the low-variance / high-persistence state quite accurately, even at the shortest simulation lengths. Yet, both models performs poorly on the high-variance / low-persistence state for simulation lengths below 1000. This is most likely explained by the fact that, at low simulation lengths the probability of seeing both states are quite low. For a series of 250 observations the probability of only seeing one state is 51\%, at 500 observations it is 30\%, for 1000 observations it is 10\% and at 2000 observations it is 1\%\footnote
{These probability are calculated using the fact that sojourn times follows a geometric distrubution.
}. 
When only one state is observed, it becomes inherently harder for the estimator to detect that only one state is actually present, when it is designed to fit two.

Previous studies such as Zheng et al. (2019), alleviated this problem by excluding those simulations with only one state present, however as noted by Nystrup et al. (2020) this introduces bias in the transition probabilities. The problem with excluding those simulations, is that when training HMMs on real data, then it possible that only one state will be present at times. This is especially true when one is considering a rolling window. In such cases, it is important that the estimator doesn't break down, which for example, is regularized with the jump penalty in the jump estimator. For most parameters, both models converge toward the true parameters around 1000 and 2000 observations, though convergence speed quickly decreases for sequences longer than that.

In conclusion, the different estimators generally converge towards the true parameter values when the simulation length is increased. However, these results are dependent on the assumption that the underlying distribution of data is known, which is generally unrealistic - especially with financial returns. In the following section we show how the estimators perform when they are purposefully estimated on an unknown distribution, which is not a mixture of gaussian distributions.


\begin{table}[H]
\centering
\caption{Estimates of HMM models' convergence towards true values as a function of simulation length. Results are based on 1000 simulations from conditional gaussian distributions.}
\input{analysis/model_convergence/output_data/simulation_normal}
\label{tab:jump_gaussian}
\end{table}


\subsection{Simulation study with misspecified distributions}

\textbf{Vis om man kan fitte de simulerede t-fordelinger mde normalfordelinger, under den antagelse at man kender de rigtige stater pÃ¥ forhÃ¥nd.}

In this section, the simulation procedure from before is repeated, except for that fact that observations are sampled from conditional student's t distribution instead of gaussian, though means, variances and transition probabilities remain unchanged. The simulated t-distribution has five degrees of freedom. Student's t distribution generally have fatter tails, and have been shown to better resemble financial returns compared to normal distributions (Cont, 2001). As such, the goal of this section is to create a more realistic simulation experiment, in which models are still specified to be conditional gaussian, even though the simulated data is not. When implementing these types of models in a portfolio, it is important to know how it performs in such an environment. The results in this section will thus provide insights into how well each estimator performs in such a scenario, and where they fail.

\cref{fig:jump_t} shows the mean parameter values as a function of simulation length based on 1000 simulations. Compared to \cref{fig:jump_normal}, the estimators performance has generally worsened. Yet, the jump estimator is generally more robust, especially when analyzing the transition probabilities, in which the jump estimator has close to similar performance as before. As mentioned in \cref{subsection: Jump theory}, a large advantage of the jump model is that, during training it does not assume anything about the distributional properties of the data, therefore making it more robust when the data's distribution is unknown.

\begin{figure}[H] 
    \centering
    \includegraphics[width=1\textwidth]{analysis/model_convergence/images/simulation_t.png}
    \caption{Estimates of HMM models' convergence towards true values as a function of simulation length. Results are based on 1000 simulations with conditional t distributions with five degrees of freedom.}
    \label{fig:jump_t}
\end{figure}

This is in contrast to the MLE model, which assumes a given conditional distribution for each state during training. From \cref{fig:jump_t} it becomes clear that such an assumption has a sizeable error as the model generally doesn't perform well across the parameters. Most conspicuously, the MLE model has basically broken down in terms of the transition probabilities where it is very far from the true estimates. This would result in states with very low persistence and frequent state switching. Though it does seem to be converging slowly toward the true parameter values, it is much less robust than the jump model. However, even if the model did eventually converge, given enough data, the problem is that financial returns are generally shown to have time varying features, (Ryden (1997), Nystrup et al. (2017)). As a result, one should be reluctant to consider estimating models on data that goes back too far, which is why the longest simulation length is 2000 observations.

In conclusion, testing the models' performance on data with misspecified distributions, has allowed us better understand how they might perform on real data, when the true values are unknown. The fact that the jump model was shown to be very robust in terms of transition probabilities, is convincing of its merit on real data. However, as a final remark it should be noted that the results presented in this section is entirely based on data simulated from an HMM, thus with the underlying assumption that financial data to some degree can be explained by such a model. As a result, the next section will consist of an analysis of HMMs ability to reproduce stylized facts. This should provide further insight into their appropriateness as times series models.

\begin{table}[H]
\centering
\caption{Estimates of HMM models' convergence towards true values as a function of simulation length. Results are based on 1000 simulations with conditional t distributions with five degrees of freedom.}
\input{analysis/model_convergence/output_data/simulation_t}
\end{table}