\newpage
 \section{Data model}
As highlighted throughout section \ref{section: Data}, a traditional Gaussian distribution provides a poor fit in regards to financial return series. In addition, when applying a variety of different predictive techniques to financial return series most of the models assume stationarity. However, Hamilton (1989) found that financial returns are often characterised as non-stationary across time due to changing means and heteroscedasticity. It should be noted that despite the fact that stationarity might not be fulfilled across long time horizons, the condition is often fulfilled over temporary periods (Hamilton, 1989). As such, mixtures of Gaussian distributions provide an enhanced fit since these distributions are better at reproducing the aforementioned stylized facts both in terms of leptokurtosis and skewness. In addition, a study conducted by Ryden et al. (1998) found that HMMs are particularly suitable for reproducing stylized facts. The study estimated 10 HMMs each on a different subset of data containing 1700 observations. Nystrup et al. (2017) later confirmed and expanded these findings by considering a rolling window of equal length from the 1930s until the 2010s.

The fundamental aspect of an HMM is that the underlying distribution that generates an observation is dependent on the state generated by an unobserved Markov chain. In addition, the transition probabilities, which determine the probability of which a state change occurs, are assumed to be constant, thereby implying that the sojourn times, i.e. the amount of time spend in each state, are geometrically distributed. However, since economic history prescribe that a minor or major recession on average occurs every seven years, the memoryless property of the geometric distribution is not always appropriate. An alternative approach would be to model the economic regimes through a Hidden Semi Markov Model (HSMM), which allows for explicit modelling of the sojourn time distributions. 

The theory of HMMs in discrete time as well as the mathematics underlying their estimation procedure is outlined through section \ref{subsection: HMM} to \ref{subsection: Decoding}. In addition, the thesis will be exploring a model estimation method, first noted by Nystrup et. al (2020), in which the jump framework of Bemporad et al. (2018) is combined with the temporal features used by Zheng et al. (2019), in order to estimate HMMs. This methodology is known as the \textit{jump estimator} and will be reviewed in section \ref{subsection: Jump theory}. As such, the jump framework serves as an alternative to the well established maximum likelihood estimation (MLE) approach. 


\subsection{Hidden Markov Models}
\label{subsection: HMM}

\textbf{Still missing description of stationary distribution and model initialization}

Hidden Markov Models, are probabilistic models whose primary function is to predict sequences of unobserved (hidden) states from a set of observed variables. The observed variables could be a log return series of a financial asset or index like the S\&P 500. The modelâ€™s inference of hidden states is based on an underlying Markov process. Therefore, as opposed to other traditional models which assume independence among observations, Markov models assumes that the sequence of observations and the associated classifications are dependent. Furthermore, the Markov property states that the classification of a state is only dependent on the former classification i.e. the previous instance. 

Let $\K=\{ 1,\ldots, K \}$ denote the state space with $K$ states, and let $\S=(s_1,s_2,\ldots,s_T)$ be a sequence of latent (unobservable) discrete random variables where $s_t\in\K: t\in \mathbb{N}$ is the state active at time $t$. Then, $s_t$ is assumed to follow a first order Markov chain where, for all $t \in \mathbb{N}$, the future state at $t + 1$ solely depends on the current state, fulfilling the Markov property. The first order Markov property is expressed through equation \ref{eq:model_markov_property}

\begin{equation}
    Pr(s_t | s_{t-1},\ldots,s_1) = Pr(s_t | s_{t-1}),
    \quad t=2,\ldots,T
    \label{eq:model_markov_property}
\end{equation} 

As such, the Markov property provide a neat mathematical simplification, since if one were to determine the probability that the current state $s_t$ is equal to some state $i$, it would require knowledge of all the states that have come before $s_t$. Given the Markov property, this knowledge can be reduced to only knowing the previous state $s_{t-1}$. The switching behaviour of $s_t$ is governed by the conditional probabilities $Pr(s_{t+1} = j| s_t = i) = q_{ij}$, where $q_{ij}$ is the probability of switching from state $i$ to state $j$ (Bulla et al., 2011). These are referred to as transition probabilities and they are stored in a transition probability matrix denoted as $Q = \{q_{ij}\} \in \mathbb{R}^{K \times K}$. The initial distribution of the transition probability matrix is stored under the variable $P(s_1|O)= \delta$.

Following the definition of the transition probabilities as well as the initial probability matrix of observing a particular state, it is important to define the the observable data. As such, define $o_t \in \mathbb{R}$, as a random variable of the observed data at time $t$ and let $O=(o_1,\ldots,o_t)$ be the sequence of observed data. Closely connected to the defined observable data lies the assumption that HMMs operate under output independence, as expressed in equation \ref{eq: Output independence}.
\begin{equation}
    Pr(o_t|O, \S) = Pr(o_t|s_t),
    \quad t=2,\ldots,T
    \label{eq: Output independence}
\end{equation}
As a result of the output independence, the distribution of the observable data depends only on the current state, thereby making the autocorrelation functions highly dependent on the persistence of $s_t$. Lastly, the emission probabilities are defined as
\begin{equation}
    B_t(o_t)=\{ b_j(o_t) \}=Pr(o_t|s_t=j), \quad \forall j \in \K, t=1,\ldots,T 
\end{equation}
where $B_t(\cdot)\in \mathbb{R}^{K\times K}$ is a diagonal matrix with state dependent probabilities, i.e. the probability of observing $o_t$ at time $t$ from state $j$. Following this acknowledgment, the sequence state dependent probabilities are defined as $B=(B_1,\ldots,B_T)$. In addition, it should be noted that when the state dependent distribution is continuous, $b_j(o_t)$ represents conditional densities rather than conditional probabilities (Jurafsky Martin, 2019). Since the HMMs utilized in this thesis will be discrete the state dependent distribution will be expressed as conditional probabilities rather than conditional densities. Conclusively, an HMM is a state-space model with finite state space, in which equation \ref{eq:model_markov_property} is the state equation and equation \ref{eq: Output independence} is the observation equation. A specific observation can potentially arise from more than one state, since the conditional distributions between the states can overlap.

\textbf{Insert a summary of all defined variables in either a table or by using bullet points.}

\subsection{A 2-state Hidden Markov Model}
Having defined the variables underlying discrete HMMs, the following section will introduce a 2-state HMM. It should be clear that the example could easily be expanded to include more states, however, the direct link between a 2-state model and the different macroeconomic environments represented as either expansions or recessions makes a 2-state model intuitive and feasible. Consider the 2-state model with Gaussian conditional distributions specifications
\begin{equation}
     o_t|s_t \sim N(\mu_{s_t},\sigma^2_{s_t}) 
\end{equation}
\begin{equation}
   o_t = \mu_{s_t}  + \sigma_{s_t}\epsilon_{t}
   ,\quad \epsilon_{t} \sim N(0,1)   
\end{equation}
where,

$$
    \mu_{s_t}=
    \begin{cases}
        \mu_1, & \text{if}\ s_t = 1 \\
        \mu_2, & \text{if}\ s_t = 2
    \end{cases},
    \sigma_{s_t} =
    \begin{cases}
        \sigma_1, & \text{if}\ s_t = 1 \\
        \sigma_2, & \text{if}\ s_t = 2
    \end{cases},
    Q = 
    \begin{bmatrix}
    q_{11} & q_{12} \\
    q_{21} & q_{22}
    \end{bmatrix},
    \Theta = (Q, B, \delta)
$$
in which $\Theta$ captures the estimated model parameters defined in section \ref{subsection: HMM}. The nature of the workings of an HMM is quite intuitive. For every period $t$, the model observes a realization $o_t$ from the current conditional distribution. As such, one can think of this as drawing an observation form an underlying distribution, however, it is unknown whether $s_t = 1$ or $2$. Further, the properties underlying the transition probabilities in $Q$ are also unknown. As such, the objective of the HMM is to estimate the most likely sequence of states $\S$, as well as the most likely parameters, $Q$, $B$ \& $\delta$. Furthermore, as previously mentioned, the sojourn times are implicitly assumed to be geometrically distributed as per equation \ref{eq: sojourn time}
\begin{equation}
    Pr('\text{staying $t$ time steps in state $i$'}) = q^{t-1}_{ii} *(1-q_{ii})
    \label{eq: sojourn time}
\end{equation}
As such, the expected duration of state \textit{i} can be depicted as 
\begin{equation}
    E[r_i] = \frac{1}{1-q_{ii}}
    \label{eq: geometric distribution memory}
\end{equation}

As evident by equation \ref{eq: sojourn time} and \ref{eq: geometric distribution memory}, the geometric distribution of the sojourn times is memoryless, implying that the time until one observes a transition out of the current state is independent of the amount of time spend in the current state. This is one of the apparent weaknesses when utilizing HMMs for modelling regime-switching behavior in economics, since macroeconomic variables are not memoryless. Lastly, it should be clear that the mathematics associated with HMMs can become quite complicated hence one of the challenging aspects associated is estimating the model parameters. This is discussed in the following section.

