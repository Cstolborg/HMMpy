\newpage
 \section{Data model}
 
 \textbf{Skriv introduktionen her om. Særligt nogle af beskrivelserne i anden paragraf er for tekniske og bør ikke gennemgås førend de er defineret nede i afsnit 3.1. Der kan godt være en mere high-level intro til HMMs, deres antagelser og brugsområder her istedet.}
Real-world processes generally produce directly observable output which can be characterized as signals (Rabiner, 1989). As an example a signal could be the different letters from an alphabet or, as is the case of this thesis, prices from a financial index like the S\&P 500. These signals can be discrete, continous, stationary, non-stationary, hence it is of fundamental interest to charactise the signals through the use of signal models. There are several possible choices for what type of signal model that is to be used for characterising the properties of a given signal (Rabiner, 1989). Typically, the selected model either stems from a class of deterministic models or statistical models. As such, the deterministic model generally exploit a known specific property of the signal, for instance the signal behaves like a sine wave. However, this type of model is not appropriate in this application since financial returns do not behave in such nice properties. On the other hand, the statistical models which include Gaussian and Markov processes as well as hidden Markov processes among others, operate under the assumption that the signal can be characterised as a parametric random process  and the parameters of the stochastic process can be estimated (Rabiner, 1989). Given these assumptions and properties, the hidden Markov model should serve as a prime data model for capturing the stochastic nature of financial returns.

The hidden Markov model (HMM) was originally introduced by Baum \& Petrie (1966) and later by Baum et al. (1970) after which the model has been applied in many fields, such as biology (Durbin et al. 1998), environmental time series (MacDonald and Zucchini 1997) as well as speech recognition (Rabiner 1989). 

Applications related to
financial econometrics followed mainly after the seminal
works of Hamilton (1989, 1990) on Markov-switching
models (a synonym for the HMM). 
 
 
As highlighted throughout section \ref{section: Data}, a traditional Gaussian distribution provides a poor fit in regards to financial return series. In addition, when applying a variety of different predictive techniques to financial return series most of the models assume stationarity. However, Hamilton (1989) found that financial returns are often characterised as non-stationary across time due to changing means and heteroscedasticity. It should be noted that despite the fact that stationarity might not be fulfilled across long time horizons, the condition is often fulfilled over temporary periods (Hamilton, 1989). As such, mixtures of Gaussian distributions provide an enhanced fit since these distributions are better at reproducing the aforementioned stylized facts both in terms of leptokurtosis and skewness. In addition, a study conducted by Ryden et al. (1998) found that HMMs are particularly suitable for reproducing stylized facts. The study estimated 10 HMMs each on a different subset of data containing 1700 observations. Nystrup et al. (2017) later confirmed and expanded these findings by considering a rolling window of equal length with observations from the 1930s until the 2010s.

The fundamental aspect of an HMM is that the underlying distribution that generates an observation is dependent on the state generated by an unobserved Markov chain. In addition, the transition probabilities, which determine the probability of which a state change occurs, are assumed to be constant, thereby implying that the sojourn times, i.e. the amount of time spend in each state, are geometrically distributed.

The theory of HMMs in discrete time as well as the mathematics underlying their estimation procedure is outlined through section \ref{subsection: HMM} to \ref{subsection: Decoding}. In addition, the thesis will be exploring a model estimation method, in which the jump framework of Bemporad et al. (2018) is combined with the temporal features used by Zheng et al. (2019), in order to estimate HMMs. This methodology is known as the \textit{jump estimator} and will be reviewed in section \ref{subsection: Jump theory}. As such, the jump framework serves as an alternative to the well established maximum likelihood estimation (MLE) approach. 


\subsection{Hidden Markov Models}
\label{subsection: HMM}

\textbf{Still missing description of stationary distribution and model initialization}

Hidden Markov Models, are probabilistic models whose primary function is to predict sequences of unobserved (hidden) states from a set of observed variables. The observed variables could be a log return series of a financial asset or index like the S\&P 500. The model’s inference of hidden states is based on an underlying Markov process. Therefore, as opposed to other traditional models which assume independence among observations, Markov models assumes that the sequence of observations and the associated classifications are dependent. Furthermore, the Markov property states that the classification of a state is only dependent on the former classification i.e. the previous instance. 

Let $\K=\{ 1,\ldots, K \}$ denote the state space with $K$ states, and let $\S=(s_1,s_2,\ldots,s_T)$ be a sequence of latent (unobservable) discrete random variables where $s_t\in\K: t\in \mathbb{N}$ is the state active at time $t$. Then, $s_t$ is assumed to follow a first order Markov chain where, for all $t \in \mathbb{N}$, the future state at $t + 1$ solely depends on the current state, fulfilling the Markov property. The first order Markov property is expressed through equation \ref{eq:model_markov_property}
\begin{equation}
    Pr(s_t | s_{t-1},\ldots,s_1) = Pr(s_t | s_{t-1}),
    \quad t=2,\ldots,T
    \label{eq:model_markov_property}
\end{equation} 

As such, the Markov property provide a neat mathematical simplification, since if one were to determine the probability that the current state $s_t$ is equal to some state $i$, it would require knowledge of all the states that have come before $s_t$. Given the Markov property, this knowledge can be reduced to only knowing the previous state $s_{t-1}$. The switching behaviour of $s_t$ is governed by the conditional probabilities $Pr(s_{t+1} = j| s_t = i) = q_{ij}$, where $q_{ij}$ is the probability of switching from state $i$ to state $j$ (Bulla et al., 2011). These are referred to as transition probabilities and they are stored in a transition probability matrix denoted as $Q = \{q_{ij}\} \in \mathbb{R}^{K \times K}$. The initial distribution of the transition probability matrix is stored under the variable $P(s_1|O)= \delta$.

Following the definition of the transition probabilities as well as the initial probability matrix of observing a particular state, it is important to define the the observable data. As such, define $o_t \in \mathbb{R}$, as a random variable of the observed data at time $t$ and let $O=(o_1,\ldots,o_t)$ be the sequence of observed data. Closely connected to the defined observable data lies the assumption that HMMs operate under output independence, as expressed in equation \ref{eq: Output independence}.
\begin{equation}
    Pr(o_t|O, \S) = Pr(o_t|s_t),
    \quad t=2,\ldots,T
    \label{eq: Output independence}
\end{equation}
As a result of the output independence, the distribution of the observable data depends only on the current state, thereby making the autocorrelation functions highly dependent on the persistence of $s_t$. Lastly, the emission probabilities are defined as
\begin{equation}
    B_t(o_t)=\{ b_j(o_t) \}=Pr(o_t|s_t=j), \quad \forall j \in \K, t=1,\ldots,T 
\end{equation}
where $B_t(\cdot)\in \mathbb{R}^{K\times K}$ is a diagonal matrix with state dependent probabilities, i.e. the probability of observing $o_t$ at time $t$ from state $j$. Following this acknowledgment, the sequence state dependent probabilities are defined as $B=(B_1,\ldots,B_T)$. In addition, it should be noted that when the state dependent distribution is continuous, $b_j(o_t)$ represents conditional densities rather than conditional probabilities (Jurafsky Martin, 2019). Since the HMMs utilized in this thesis will be discrete the state dependent distribution will be expressed as conditional probabilities rather than conditional densities. Conclusively, an HMM is a state-space model with finite state space, in which equation \ref{eq:model_markov_property} is the state equation and equation \ref{eq: Output independence} is the observation equation. A specific observation can potentially arise from more than one state, since the conditional distributions between the states can overlap.

\textbf{Insert a summary of all defined variables in either a table or by using bullet points.}

\subsection{A 2-state Hidden Markov Model}
Having defined the variables underlying discrete HMMs, the following section will introduce a 2-state HMM. It should be clear that the example could easily be expanded to include more states, however, the direct link between a 2-state model and the different macroeconomic environments represented as either expansions or recessions makes a 2-state model intuitive and feasible. Consider the 2-state model with Gaussian conditional distributions specifications
\begin{equation}
     o_t|s_t \sim N(\mu_{s_t},\sigma^2_{s_t}) 
\end{equation}
\begin{equation}
   o_t = \mu_{s_t}  + \sigma_{s_t}\epsilon_{t}
   ,\quad \epsilon_{t} \sim N(0,1)   
\end{equation}
where,

$$
    \mu_{s_t}=
    \begin{cases}
        \mu_1, & \text{if}\ s_t = 1 \\
        \mu_2, & \text{if}\ s_t = 2
    \end{cases},
    \sigma_{s_t} =
    \begin{cases}
        \sigma_1, & \text{if}\ s_t = 1 \\
        \sigma_2, & \text{if}\ s_t = 2
    \end{cases},
    Q = 
    \begin{bmatrix}
    q_{11} & q_{12} \\
    q_{21} & q_{22}
    \end{bmatrix},
    \Theta = (Q, B, \delta)
$$
in which $\Theta$ captures the estimated model parameters defined in section \ref{subsection: HMM}. The nature of the workings of an HMM is quite intuitive. For every period $t$, the model observes a realization $o_t$ from the current conditional distribution. As such, one can think of this as drawing an observation form an underlying distribution, however, it is unknown whether $s_t = 1$ or $2$. Further, the properties underlying the transition probabilities in $Q$ are also unknown. As such, the objective of the HMM is to estimate the most likely sequence of states $\S$, as well as the most likely parameters, $Q$, $B$ \& $\delta$. Furthermore, as previously mentioned, the sojourn times are implicitly assumed to be geometrically distributed as per equation \ref{eq: sojourn time}
\begin{equation}
    Pr('\text{staying $t$ time steps in state $i$'}) = q^{t-1}_{ii} *(1-q_{ii})
    \label{eq: sojourn time}
\end{equation}
As such, the expected duration of state \textit{i} can be depicted as 
\begin{equation}
    E[r_i] = \frac{1}{1-q_{ii}}
    \label{eq: geometric distribution memory}
\end{equation}

As evident by equation \ref{eq: sojourn time} and \ref{eq: geometric distribution memory}, the geometric distribution of the sojourn times is memoryless, implying that the time until one observes a transition out of the current state is independent of the amount of time spend in the current state. This is one of the apparent weaknesses when utilizing HMMs for modelling regime-switching behavior in economics, since macroeconomic variables are not memoryless. Lastly, it should be clear that the mathematics associated with HMMs can become quite complicated hence one of the challenging aspects associated is estimating the model parameters. This is discussed in the following section.

