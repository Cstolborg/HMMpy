\newpage
 \section{Data model}
As highlighted throughout section \ref{section: Data}, a traditional Gaussian distribution provides a poor fit in regards to financial return series. In addition, when applying a variety of different predictive techniques to financial return series most of the models assume stationarity. However, Hamilton (1989) found that financial returns are often characterised as non-stationary across time due to changing means and heteroscedasticity. It should be noted that despite the fact that stationarity might not be fulfilled across long time horizons, the condition is often fulfilled over temporary periods (Hamilton, 1989). Instead mixtures of Gaussian distributions provide an enhanced fit, since these distributions are better at reproducing the aforementioned stylized facts both in terms of leptokurtosis and skewness. Ryden et al. (1998) found HMM's suitable for reproducing stylized facts on 10 different subsets of approximately 1,700 observations and Nystrup et al. (2017) expanded these findings by considering a rolling window of equal length from the 1930s until the 2010s.

The fundamental aspect of an HMM is that the underlying distribution that generates an observation is dependent on the state generated by an unobserved Markov chain. In addition, the transition probabilities, which determines the probability of which a state change occurs, are assumed to be constant, thereby implying that the sojourn times, i.e. the amount of time spend in each state, are geometrically distributed. However, since the thesis is centered around modelling of economic regimes, and economic theory prescribe that a minor or major recession on average occurs every seven years, the memoryless property of the geometric distribution is not always appropriate. The limitation is somewhat circumvented by relying on global stock indices, such as the MSCI World, as well as by setting a limitation on the variation of the amount of different economic regimes. An alternative approach would be to model the economic regimes thorough a Hidden Semi Markov Model (HSMM), which allows for an explicit modelling of the sojourn time distributions. 

The theory of HMMs in discrete time as well as their associated mathematics and estimation is outlined through section \ref{subsection: HMM} to \ref{subsection: Decoding}. In addition, the thesis will be exploring a model estimation method, first noted by Nystrup et. al (2020) in which the jump framework of Bemporad et al. (2018) is combined with the temporal features used by Zheng et al. (2019), in order to estimate HMMs. This methodology is known as the \textit{jump estimator} and will be reviewed in section \ref{subsection: Jump theory}. As such, the jump framework serves as an alternative to the classical maximum likelihood estimation (MLE) approach. 



\subsection{Hidden Markov Models}
\label{subsection: HMM}

\textbf{Still missing description of stationary distribution and model initialization}

Hidden Markov Models, are probabilistic models whose primary function is to predict sequences of unobserved (hidden) states from a set of observed variables. The observed variables could be a price series of a financial asset like the MSCI World index.

The modelâ€™s inference of hidden states is based on an underlying Markov process. As such, as opposed to other traditional model which assume independence among observations, Markov models assumes that the sequence of observations and the associated classifications are dependent. Furthermore, the Markov property states that the classification, which encompasses the prediction of a state, is only dependent on the former classification i.e. the previous instance. 

Let $\K=\{ 1,\ldots, K \}$ denote the state space with $K$ states, and let $\S=(s_1,s_2,\ldots,s_T)$ be a sequence of latent (unobservable) discrete random variables where $s_t\in\K: t\in \mathbb{N}$ is the state active at time t. Then, $s_t$ is assumed to follow a first order Markov chain, where for all $t \in \mathbb{N}$ the future state at $t + 1$ solely depends on the current state, fulfilling the Markov property

\begin{equation}
    Pr(s_t | s_{t-1},\ldots,s_1) = Pr(s_t | s_{t-1}),
    \quad t=2,\ldots,T
    \label{eq:model_markov_property}
\end{equation} 

As such, the Markov property provide a neat mathematical simplification, since if one were to determine the probability that the current state $s_t$ is equal to some state $i$ from a full probabilistic system, it would require knowledge of all the states that have come before $s_t$. Given the Markov property, this knowledge can be reduced to only knowing the current state $S_t$. 

The switching behaviour of $s_t$ is governed by the conditional probabilities $Pr(s_{t+1} = j| s_t = i) = q_{ij}$, where $q_{ij}$ is the probability of switching from state $i$ to state $j$ (Bulla et al., 2011). These are referred to as transition probabilities and they are stored in a transition probability matrix denoted as $Q = \{q_{ij}\}$. The initial distribution of the transition probability matrix is stored under the variable $P(s_1|O)= \delta$.

Define emissions (observed data) as $o_t \in \mathbb{R}$, a random variable of observed data at time step t and let $O=(o_1,\ldots,o_t)$ be the sequence of observed data. HMMs operate under the assumption of output independence, expressed as
\begin{equation}
    Pr(o_t|O, \S) = Pr(o_t|s_t),
    \quad t=2,\ldots,T
    \label{eq: Output independence}
\end{equation}
As a result of the output independence, the distribution of the observable data depends only on the current state, thereby making the autocorrelation functions highly dependent on the persistence of $s_t$. In addition, we define emission probabilities as
\begin{equation}
    B_t(o_t)=\{ b_j(o_t) \}=Pr(o_t|s_t=j), \quad \forall j \in \K, t=1,\ldots,T 
\end{equation}
where $B_t(\cdot)\in \mathbb{R}^{K\times K}$ is a diagonal matrix with state dependent probabilities, i.e. the probability of observing $o_t$ at time t from state $j$. Further, we denote $B=(B_1,\ldots,B_T)$ as the sequence state dependent probabilities. When the considered distribution is continuous, $b_j(o_t)$ instead of conditional probabilities, represents conditional densities (Jurafsky Martin, 2019), which is the case in this study where we shall mostly consider conditional normal distributions.

An HMM is a state-space model with finite state space where equation \ref{eq:model_markov_property} is the state equation and equation \ref{eq: Output independence} is the observation equation. A specific observation can potentially arise from more than one state, since the conditional distributions between the states can overlap.

\subsection{A 2-state Hidden Markov Model}

In the following section, a 2-state HMM is introduced, however, the example could easily be expanded to include more states. Consider the two-state model with Gaussian conditional distributions specifications:
\begin{equation}
     o_t|s_t \sim N(\mu_{s_t},\sigma^2_{s_t}) 
\end{equation}
\begin{equation}
   o_t = \mu_{s_t}  + \sigma_{s_t}\epsilon_{t}
   ,\quad \epsilon_{t} \sim N(0,1)   
\end{equation}
where,

$$
    \mu_{s_t}=
    \begin{cases}
        \mu_1, & \text{if}\ s_t = 1 \\
        \mu_2, & \text{if}\ s_t = 2
    \end{cases},
    \sigma_{s_t} =
    \begin{cases}
        \sigma_1, & \text{if}\ s_t = 1 \\
        \sigma_2, & \text{if}\ s_t = 2
    \end{cases},
    Q = 
    \begin{bmatrix}
    q_{11} & q_{12} \\
    q_{21} & q_{22}
    \end{bmatrix},
    \Theta = (Q, B, \delta)
$$

Where $\Theta$ captures the estimated model parameters. The nature of the workings of a HMM are quite intuitive to comprehend. For every period t, the model observes a realization $o_t$ from the current conditional distribution. As such one can think of this as drawing an observation form an underlying distribution. However, it is unknown whether $s_t = 1$ or $2$. Further, the properties underlying the transition probabilities in $Q$ are also unknown. As such, the objective of the HMM is to estimate the most likely sequence of $s_t$ as well as the most likely parameters, $Q$ \& $B$. 

Furthermore, as previously mentioned, the sojourn times are implicitly assumed to be geometrically distributed as per equation \ref{eq: sojourn time}
\begin{equation}
    Pr('\text{staying t time steps in state i'}) = q^{t-1}_{ii} *(1-q_{ii})
    \label{eq: sojourn time}
\end{equation}
As such, the expected duration of state \textit{i} can be depicted as 
\begin{equation}
    E[r_i] = \frac{1}{1-q_{ii}}
    \label{eq: geometric distribution memory}
\end{equation}

As evident by equation \ref{eq: sojourn time} and \ref{eq: geometric distribution memory}, the geometric distribution of the sojourn times is memoryless, implying that the time until one observes a transition out of the current state is independent of the amount of time spend in the current state. One of the challenging aspects associated with HMMs is estimating the model parameters. This is discussed in the following section.

