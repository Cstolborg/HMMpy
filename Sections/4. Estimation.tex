\section{Estimation}
\label{section: estimation}

\subsection{Maximum likelihood estimation}
The traditional way of estimating the parameters making up the HMM is through the Maximum Likelihood
(ML) method. Following this method brings about three problems:

    1. Determine the likelihood of which the model generated a sequence of  observations $O$. 
    2. Estimate the model parameters that maximize the likelihood of the observed data.

    3. Based on the observation sequence, infer the most likely sequence of states $\S$.
\label{subsection: MLE}
 
In order to solve the first problem, one need to rely on a dynamic programming technique, known as the forward–
backward algorithm. The second problem is solved by relying on the work by Baum et al. 1970 and Dempster et al. (1977), which involves utilizing the Baum–Welch
algorithm. Finally, the third problem is solved by using the Viterbi algorithm. All the algorithms and their respective mathematics are outlined in the next three subsections. 
 
\subsubsection{The Forward-Backward Algorithm}
\label{Section: Forward backward}
In order to motivate the use of the forward-backward algorithm, one can try to imagine all possible combinations of state sequences i.e. $\S = s_1,\ldots,s_t$. The state sequence will always have the same length as the observation sequence $O_t$. The relevant information to uncover is the probability of seeing a particular observation sequence, given a state sequence. After uncovering all the possible state sequences that can produce a particular observations sequence, the probabilities can be summed, and the answer simplifies to a likelihood. Mathematically, the likelihood ($L_T$) of the observations given the model parameters $\Theta$, can be written as expressed in equation \ref{eq: motive fb}.
\begin{equation}
    L_T = P(O_t|\Theta) = \sum_{S_1,\ldots,S_t} \delta_{s_1}*b_{s_1}*(o_1) * q_{s_1s_2} * b_{s_2}(o_2)*q_{s_2s_3} .....q_{s_t-1 s_t}*b_{s_t}*(o_t)
    \label{eq: motive fb}
\end{equation}

As expressed through equation \ref{eq: motive fb}, if an observations sequence is assumed to consist of 10 observations, which can be generated by a variety of different state sequences, one must derive the individual probability of each potential state sequence that can generate the correct observation sequence and sum them up to find the probability of  an observation sequence $O_t$ given a model $\Theta$. The problem with this approach is that when the number of states ($m$) and the number of observations ($T$) grows large the complexity increases, and this methodology becomes highly computationally expensive. The number of operations required to estimate equation \ref{eq: motive fb} simplifies to:
\begin{equation}
    (2T-1)*m^T+(m^T-1)
\end{equation}

As such, for an observations sequence with $T = 100$ and $N = 5$, following the procedure of equation \ref{eq: motive fb} would result in $10^{72}$ operations. When the number of observations and states grows larger, this becomes even more computationally heavy, hence a better methodology is needed.  
In order to reduce the computational complexity and solve the problem of deriving the probability of a particular observation sequence, the forward–backward algorithm is used. These results will also be used in the subsequent Baum-Welch algorithm. The ith element of the vector $\mathbf{\alpha_t}$ is the forward probability.
\begin{align}
    \alpha_t(i) = P(o_1,\ldots,o_t, s_t = i | \Theta)
    \label{eq: forward prob}
\end{align}

As such, it is evident from equation \ref{eq: forward prob} that the forward trellis can be defined as the probability of seeing observation $o_1 $ to $o_t$ while ending up at state $i$ at time $t$. Therefore, the forward probability $\alpha_t(i)$ will be derived for each possible state at each time $t$. This means that the derivation of the forward probability can be separated in a base, inductive and termination step as depicted by equation \ref{eq: base case forward prob}, \ref{eq: inductive step forward prob} and \ref{eq: termination step}.

\begin{equation}
    \alpha_1(i) = \delta_i*b_i(o_1),
    \quad  \forall i \in \K
    \label{eq: base case forward prob}
\end{equation}
\begin{equation}
    \alpha_{t+1}(j) = \Big[\sum_{i=1}^K \alpha_t(i)*q_{ij}\Big] * b_j(o_{t+1}),
    \quad 1\leq t \leq T,
     \forall i,j \in \K
    \label{eq: inductive step forward prob}
\end{equation}
\begin{equation}
    L_T = P(O|\Theta) = \sum_{i=1}^K a_T(i)
    \label{eq: termination step}
\end{equation}

The base case represents the probability of starting in state $i$ while observing $o_1$. After this initial calculation, which is done for all states, the inductive step is initialized. The inductive step derives the joint probability of seeing state $j$ at $t+1$ and observing the sequence $o_1,\ldots,o_t$. As such, the forward probabilities derived one time step before is stored, and each of these stored values are then multiplied with their transition probability of jumping to state $j$ at time $t+1$. Taking the sum of all the possible paths and multiplying with the emission probability of seeing $o_{t+1}$ yields $\alpha_{t+1}(j)$. Therefore, the forward probabilities are inductively rolled forward until termination is reached at $T$. At termination the forward probabilities are summed across all states and a single value is reached which encompass the probability of observing a particular observation sequence based on a given set of model parameters $\Theta$.

The method depicted through equation \ref{eq: base case forward prob}, \ref{eq: inductive step forward prob} and \ref{eq: termination step} provides an intuitive inductive framework as opposed to the more comprehensive methodology introduced by equation \ref{eq: motive fb}. Furthermore, by introducing the forward probability framework, the computational complexity reduces to $O(m^2T)$. This reduction in mathematical operations makes it appropriate to implement the algorithm in an operationally efficient manner, hence the methodology could be applied by institutional investors and fund managers. 

Although the forward probability is all one need to solve the first problem in order to determine the probability of seeing an observation sequence $o_t$ given a model $\Theta$, the same methodology will be conducted backward in order to solve additional problems related to the Baum-Welch algorithm. The backward probability, denoted $\beta_t(i)$ is derived in a similar fashion to the forward algorithm defined in equation \ref{eq: forward prob}.

\begin{equation}
    \beta_t(i) = P(o_{t+1},\ldots,o_T | s_t = i, \Theta) 
\end{equation}

The primary difference between the forward and backward probabilities is that instead of looking at an observation sequence from the past up to $t$, the backward probability determines the probability of seeing a future observation sequence from $t+1$ to $T$ given that the model is in state $i$ at time $t$. As such, the backward probability is reciprocal to the forward probabilities. Similar to the forward probability one can separate the backward probabilities into a base and inductive step. 
\begin{equation}
    \beta_T(i) = 1 % 1\leq i \leq m%
    \label{eq: base backward}
\end{equation}
\begin{equation}
   \beta_t(i) = \sum_{j=1}^K q_{ij}*b_j(o_{t+1})*\beta_{t+1}(j),
   \quad t = T-1,\ldots,1,
   \quad \forall i,j \in\K
    \label{eq: Inductive backward}
\end{equation}

It is evident from equation \ref{eq: base backward} and \ref{eq: Inductive backward} that the backward probabilities are derived by summing over all of the states that the model might end up while taking into account the transition and emission probabilities as well as the $\beta$ at time $t+1$.

From the forward and backward probabilities it follows that the probability of observing a particular sequence O and being in state $i$ at time $t$ can be found as
\begin{align}
    Pr(o_1,\ldots,o_t, s_t = i | \Theta) * 
    Pr(o_{t+1},\ldots,o_T | s_t = i, \Theta)
     &=\alpha_t(i) * \beta_t(i) \\
     &= Pr(O, s_t = i |\Theta)  \notag
\end{align}

As such, the likelihood of the observed data $O$ can be evaluated by marginalizing over all $K$ states:
\begin{equation}
    Pr(O | \Theta) = \sum_{i=1}^K \alpha_t(i) * \beta_t(i)
    \label{eq: solution problem 1}
\end{equation}

As such, one can see that equation \ref{eq: termination step} and \ref{eq: solution problem 1} provide identical results when $t=T$. In relation to the implementation forward-backward algorithm, it is necessary to scale the probabilities to avoid numerical
underflow, as described in Zucchini and MacDonald (2009). 

\subsubsection{The Baum-Welch Algorithm}
There are two popular methodologies to maximizing the likelihood which involves the direct nummerical maximization and the Baum-Welch algorithm, which is a unique case of the more general expectation-maximization (EM) algorithm developed by Baum et al. (1970). Of the two, the Baum-Welch algorithm is often preferred due to its larger robustness to initial starting values (Nystrup, 2014). Yet, the starting values are crucial since even though the likelihood is guaranteed to increase or remain the same for each iteration of the EM algorithm, the convergence towards the global maximum might be slow, or worse, it might get stuck at a local maximum. In order to circumvent this, the paper relies on several runs of the EM algorithm with different starting values, thereby making it possible to uncover any significant deviations in the maximum likelihood. 

The Baum–Welch algorithm maximizes the logarithm of the complete-data likelihood (CDLL), which includes the log-likelihood of the observations $O$ and the hidden data $\S$. This can be written as

\begin{equation}
    \log(Pr(O, \S) = \log(\delta_{s_1} * \Pi_{t=2}^T q_{s_{t-1},s_t} * \Pi_{t=1}^T b_{s_t}(o_t)) 
\end{equation}
This can also be expressed as
\begin{equation}
    \log(Pr(O,\S) = \log(\delta_{s_1}) + \sum_{t=2}^T \log(q_{s_{t-1},s_t})+\sum_{t=1}^T log(b_{st}(o_t)))
\end{equation}

In order to simplify the operational nature of the maximum likelihood function, the following binary random variables are introduced below.
$$
\gamma_i(t) = 1 \ \text{if and only if}\ s_t = i
$$
and
$$
\xi_{ij}(t) = 1 \ \text{if and only if}\ s_{t-1}=i\ \text{and}\ s_t = j
$$
As such if no state transition occurs $\gamma_i(t)$ is activated and if a state transition occurs $\xi_{ij}(t)$ is activated. The CDLL can now be written as

\begin{equation}
    \log(Pr(O, \S)) = \sum_{i=1}^K \gamma_i(1) \log\delta_i + \sum_{i=1}^K \sum_{j=1}^K \Big(\sum_{t=2}^T \xi_{ij}(t)\Big) \log q_{ij}+ \sum_{i=1}^K \sum_{t=1}^T \gamma_i(t) \log b_i(o_t) 
    \label{eq: CDLL}
\end{equation}

Now, the idea underlying the methodology of the EM algorithm is to replace the quantities of $\gamma_i(t)$ and $\xi_{ij}(t)$ by their conditional expectations given an observation sequence $O$ as well as the current model parameters $\Theta(Q, B,\delta)$. This is referred to as the estimation-step (E-step) (Nystrup, 2014). 

\begin{equation}
    \hat{\gamma_i(t)} = Pr(s_t=i | O) = \frac{\alpha_i(t)*\beta_i(t)}{\sum_{j=1}^K \alpha_t(j)\beta_t(j)} = \frac{\alpha_i(t)*\beta_i(t)}{P(O|\Theta)}
    \label{eq: gamma}
\end{equation}
and
\begin{align}
    \hat{\xi_{ij}(t)} = Pr(s_{t-1}=i, s_t=j|o^{(T)}) &= \label{eq: xi} \\ 
    \frac{\alpha_{t-1}(i)*q_{ij}b_j(o_{t+1}\beta_{t+1}(j)}
            {\sum_{i=1}^K\sum_{j=1}^K\alpha_{t-1}(i)*q_{ij}b_j(o_{t+1}\beta_{t+1}(j)}
    &= \frac{\alpha_{t-1}(i)*q_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{P(O|\Theta)}  \notag
\end{align}

As such, it is evident $\gamma_i(t)$ encompass the probability of being in state $i$ at time $t$ knowing all the observations that has come before and all the observations that is to come, regardless of all the possible ways the model could have arrived at $t$ as well as all the possible ways the model can continue after $t$. Given this analogy one can think of $\gamma_i(t)$ as a bowtie that ties the forward and backward probabilities together for a given state $i$ at a given time $t$. As such, given the properties in equation \ref{eq: gamma}, it is evident that $\sum_{i=1}^K \gamma_i(t) = 1$

Furthermore, $\xi_{ij}(t)$ captures the probability of being in state $i$ at time $t$ and then transitioning to state $j$ at time $t+1$, given all the past observations and all the observations to come. Therefore, the similarity to $\gamma_i(t)$ is evident, however, $\xi_{ij}(t)$ takes into account two time steps. As such, given the properties of equation \ref{eq: gamma} and \ref{eq: xi} it is clear that $\sum_{j=1}^K \xi_{ij}(t) = \gamma_i(t)$. 

Having replaced $\gamma_i(t)$ and $\xi_{ij}(t)$ by their conditional expectations $\hat{\gamma_i(t)}$ and $\hat{\xi_{ij}(t)}$ the CDLL from equation \ref{eq: CDLL} is maximized with respect to three sets of parameters which includes the initial distribution $\delta$, the transition probability matrix $Q$ as well as the parameters of the state dependent distributions captured by $B$. This process is referred to as the maximization step (M-step). 

As such, the mechanism of the EM algorithm is to repeat the E-step and M-step until one or several convergence criterion have been satisfied. This criterion could entail that the iteration proceeds until the resulting change in the log-likelihood is below some threshold. As briefly mentioned, since the EM algorithm is sensitive to starting values there is no guarantee that the model parameters converge to a global maximum. In order to avoid the model getting stuck in a local maximum or a saddle point, several starting values are tested. 

The solution set to the parameters is 
\begin{equation}
    \delta_i = \frac{\hat{\gamma_i(1)}}{\sum_{i=1}^K\hat{\gamma_i(1)}} = \hat{\gamma_i(1)}
\end{equation}
\begin{equation}
    q_{ij} = \frac{\sum_{t=1}^{T-1}\hat{\xi_{ij}(t)}}{\sum_{t=1}^{T-1}\hat{\gamma_i(t)}}
\end{equation}

where $\sum_{t=1}^{T-1}\hat{\xi_{ij}(t)}$ is the total number of expected transitions from state $i$ to state $j$ and $\sum_{t=1}^{T-1}\hat{\gamma_i(t)}$ is the total number of transitions from state $i$.

Furthermore, maximization of the third term in equation \ref{eq: CDLL} may be easy or difficult depending on the nature of the state-dependent distributions (Zucchini, 2009). In some cases numerical maximization will be necessary, but in the case of conditional univariate normal
distributions the maximizing values of the state-dependent parameters are
\begin{align}
    \hat\mu_j &= \frac{\sum_{t=1}^T\hat\gamma_j(t)o_t}{\sum_{t=1}^T \hat\gamma_j(t)} \\
    \hat\sigma_j^2 &= \frac{\sum_{t=1}^T\hat\gamma_j(t)(o_t-\hat\mu_j)^2}{\sum_{t=1}^T \hat\gamma_j(t)}
\end{align}

In order to utilize this on a large scale, one must carefully consider the speed of convergence. One way to increase the speed of convergence is through a hybrid algorithm that starts with the EM algorithm and switches to a Newton-type algorithm when a certain stopping criterion is fulfilled (Bulla \& Berzel, 2008)

It should be clear that the likelihood of an HMM is a complicated function of defined model parameters hence it has several local maxima. The goal of the algorithm is obviously to find the model parameters that maximizes the likelihood, however, there is no simple way to determine that a global maximum has been reached. As such, the paper utilizes a sensible strategy which involves using a range of starting values for the initial model parameters.

As such, the EM algorithm is circular since the algorithm initializes with a qualified estimate of the model parameters $\Theta(Q, B,\delta)$. The forward-backward algorithm then derives $\alpha_t(i), \beta_t(i), \gamma_i(t)$ and $\xi_{ij}(t)$. This is the E-step. These derived parameters are then used to update the model parameters by maximizaing the CDLL log-likelihood function. This completes one iteration of the EM algorithm and the model starts over until it converges to a local or preferably global maximum. Alternative methodologies for maximizing the likelihood function do exist. For instance a Bayesian approach based on Markov chain Monte Carlo (MCMC) sampling can be used (Frühwirth-Schnatter 2006). It follows that Rydén (2008) compared the EM and MCMC approaches and found that MCMC can be advantageous for interval estimation and inferential problems, yet the EM serves as a simpler and faster way of obtaining point estimates.

As such, the forward-backward and Baum-Welch (EM) algorithm have solved the first two problems, hence the only aspect left is to infer the most likely sequence of states. This is done through the Viterbi algorithm. 
 
\subsubsection{Decoding: The Viterbi Algorithm}
\label{subsection: Decoding}
Inferring the most likely sequence of states is referred to as decoding. This is a crucial aspect of the HMM since a particular observation sequence can produce a variety of associated state sequences since several conditional distribution can produce the same observations. Furthermore, decoding becomes important since the parameters must be optimized not just for the most probable sequence of states but the sequence of states must also be viable. For instance if the model suggests a jump from state $i$ to state $j$ at time $t$ to $t+1$, but due to constraints, such as a transition probability matrix of 0\%, this state transition may not viable in reality, hence the model must select another appropriate transition. 

As such, the model should estimate the most probable state sequence while taking into account that the particular state sequence is viable in a real-world context. In addition the decoding is essentially the information that makes it possible to construct a trading / investment strategy based on a HMM, since an investor need to know the states of the economy to allocate capital in an efficient manner. This means that a HMM without a decoding procedure becomes rather useless from an investment perspective. 

Decoding can be done either locally, by determining the most likely state at each time $t$ or globally by determining the most likely sequence of states (Nystrup, 2014). The issue with local decoding is that it deduces the state sequence at each time instead of doing so across the entire time horizon. This can lead to suggested state sequences that are impossible to take, hence global decoding is preferred. 

\begin{equation}
    \argmax_{\S}Pr(\S|O, \Theta)
    \label{eq: global decoding}
\end{equation}

Maximizing equation \ref{eq: global decoding} over all possible state sequences by brute force involves O(mT) operations and thus proves inefficient on large scale evaluations. Rather, the Viterbi algorithm can be used to compute
the most likely sequence of states in an efficient manner. 

The best way to comprehend the Viterbi algorithm is to realize that at $t+1$ the algorithm has to figure out the state at time $t$ that maximizes the probability of making a transition to a given state $i$ and observing a specific observation at time $t+1$. Once this is done, the state from time $t$ is stored. Now the algorithm moves on to time $t+2$ and find the state that maximizes a move from time $t+1$ to $t+2$ while observing a specific observation at time $t+2$. Then the state at t+1 that maximizes this is stored and so forth. Once the algorithm terminates at $T$ it will contain a map of valid state sequences that can be followed and interpreted. 

In order to implement the outlined reasoning some additional variables are introduced.

\begin{equation}
    \upsilon_t(i) =  \max_{\S}Pr(\S|O, \Theta)
\end{equation}
and 
$$
{\psi_t(i)}
$$

$\upsilon_t(i)$ finds the path with the highest probability that accounts for the first $t$ observations and ends up at state $i$ at time $t$ and $\psi_t(i)$ serves as a storing variable that keeps track of optimal state at each time step $t$. Similar to the previously described algorithms, the Viterbi algorithm can be separated into an base, inductive and termination step.
\begin{equation}
    \upsilon_1(i) = \delta_i*b_i(o_1)
    \label{eq: viterbi start 1}
\end{equation}
\begin{equation}
     \psi_1(i) = 0 
\end{equation}
\begin{equation}
    \upsilon_t(j) = \max_{i}\Big[\upsilon_{t-1}(i)q_{ij}\Big] *b_j(o_t),
    \quad t=2,\ldots,T , \quad i,j \in \K 
    \label{eq: inductive viterbi}
\end{equation}
\begin{equation}
    \psi_t(j) = {\argmax_{i} \Big[\upsilon_{t-1}(i)q_{ij}\Big]},
    \quad {t= T-1,\ldots,1}, \quad i,j\in\K
    \label{eq: inductive viterbi 2}
\end{equation}

Naturally, the best initial state is the one that maximizes the probability of observing a particular observation $o_1$ as expressed in equation \ref{eq: viterbi start 1}. Furthermore, it is obvious that the storing variable $\psi_t(i)$ is 0 at initialization since no optimal state has been discovered.

The first term in equation \ref{eq: inductive viterbi} is the state that maximized $\upsilon$ at the previous time step. In this case the state is set to $i$ but the algorithm scan over all the potential states that the model could have come from. After doing so the algorithm considers all the ways of getting from state  $i$ to $j$ which is the transition probability, which is then multiplied with the probability of observing $o_{t+1}$ at the state $j$. 

Therefore, the max function of equation \ref{eq: inductive viterbi} is going to find the state from the previous time period that maximizes the probability of moving to the current state. Note that the optimal state from the previous iteration is stored in the variable $\psi$ such that the algorithm does not loose track of previous iterations. 

The termination step of the algorithm is presented in equation \ref{eq: viterbi termination 1} and \ref{eq: viterbi termination 2}
\begin{equation}
    \upsilon^* = \max_{i}\Big[\delta_T(i)\Big],
    \quad i \in\K
    \label{eq: viterbi termination 1}
\end{equation}
 \begin{equation}
     s_T^*= \argmax_{i}\Big[\delta_T(i)\Big],
     \quad i \in\K
     \label{eq: viterbi termination 2}
 \end{equation}
Where $\upsilon^*$ expresses the highest probability for the valid sequence of states given an observation sequence. As such, knowing $\upsilon^*$ it is possible to iterate backwards through the process as
\begin{equation}
    s_t^* = \psi_{t+1}*(s_{t+1}^*)
\end{equation}

Comparing equation \ref{eq: inductive viterbi} with equation \ref{eq: inductive step forward prob} it is evident that the dynamic programming technique for the Viterbi algorithm is the same as in the forward–backward
algorithm. The only difference is the substitution of the sum with a maximization. From a computational efficiency perspective the Viterbi algorithm remains at $O(m^2T)$, thereby making it feasible for full-scale production applications (Dittmer 2008).

\subsection{Logarithmic scaling to handle numerical underflow}

The theory above have outlined the mathematics comprising HMMs, however, a crucial aspect regarding their implementation is dealing with the underflow and/or overflow produced by the algorithms. The issues regarding underflow arises when $T$ gets large since the algorithms will begin multiplying together smaller and smaller numbers when for instance deriving the forward and backward probabilities. Overflow is the inverse to underflow and it occurs when continuously larger numbers are multiplied together. As such, underflow and overflow proposes a challenge to the implementation of the HMMs as Python or other libaries will generate an error or simply interrupt the execution of the code. The challenges associated with underflow and overflow is an area that is widely overlooked in the literature, despite the fact it is of crucial importance in terms of utilizing HMMs on a full-scale operational level. This section aims at proposing a solution towards the challenges associated with underflow and overflow.

\subsubsection{Scaling the likelihood computation}
For discrete state-dependent distributions, the element of the forward probabilities $\alpha_t$ ,as seen in equation \ref{eq: inductive step forward prob}, become progressively smaller as $t$ increase because the forward probabilities are made up of products of probabilities, hence they will exponentially converge towards zero. The remedy needed to circumvent this issue is the same for overflow and underflow, hence the authors confine their attention to underflow.

A circumvention of the challenges related to underflow can be implemented by computing the logarithm of $L_T$ in equation \ref{eq: motive fb} which involves using a strategy of scaling the vector of forward and backward probabilities $\alpha_t$ and $\beta_t$. As such, a scaling of the vector of forward and backward probabilities can be conducted by log-transforming the  traditional formulas presented in section \ref{Section: Forward backward}. This methodology does not change the workings of the algorithms, however, it reduces the problem associated with underflow as the forward and backward probabilities no longer becomes excessively small when $t$ increases. Since the log-transformation will be rooted in the formulas presented in \ref{Section: Forward backward}, one can separate the transformations into a base, inductive and termination step.  

\begin{equation}
    \log\alpha_1(i) = \log\delta_i + \log b_i(o_1)
    \quad \forall i \in\K
    \label{eq: log base case forward prob}
\end{equation}

\begin{equation}
    \log\alpha_{t+1}(j) = \log\Big[\sum_{i=1}^K \exp^{\log a_t(i)+\log q_{ij}}\Big] + \log b_j(o_{t+1}),
    \quad t=2,\ldots,T-1, 
    \quad \forall j \in\K
    \label{eq: log inductive step forward prob}
\end{equation}

\begin{equation}
     L_T = \log P(O |\Theta) = \log\sum_{i=1}^K \exp^{\log\alpha_T(i)}
    \label{eq: log termination step}
\end{equation}

As should be evident by equation \ref{eq: log base case forward prob}, \ref{eq: log inductive step forward prob} and \ref{eq: log termination step} a large part of the transformation relies on the following mathematics $log(ab) = log(a) + log(b)$. Furthermore, the transformation, particularly the transformation in the inductive step from equation \ref{eq: log inductive step forward prob}, relies on the mathematics guiding the LogSumExp (LSE) function. The LSE is defined as $LSE(x_1,...,x_n) = log(exp^{(x_1)}+...+exp^{(x_n)})$. One of the most common purposes of relying on log-transformations for algorithmic computations is to increase accuracy and avoid underflow. 

Yet, the utilization of the LSE function directly, as depicted in equation \ref{eq: log inductive step forward prob}, can result in additional problems related to overflow/underflow. Therefore, the following formula, which provides equivalent results as the provided LSE formula, must be used instead. The revised LSE formula is defined as $LSE(x_1,....,x_n) = x^* + log(exp(x_1-x^*)+....+exp(x_n-x^*))$ where $x^* = max(x_1,.....,x_n)$. Despite having more than one variable, the procedure remains the same, yet the $x^*$ will be selected from a range of multiple vectors. This mathematical trick is done in order to ensure that the number of which the logarithm is taken is above 0, since the natural logarithm is undefined for values below 0. 

By utilising the logarithmic transformation formulas just presented, it is possible to conduct a similar transformation for the backward probabilities derived from equation \ref{eq: base backward} and \ref{eq: Inductive backward}. 

\begin{equation}
     \log\beta_T(i) = 0
    \label{eq: log base backward}
\end{equation}

\begin{equation}
    \beta_t(i) = \log\sum_{j=1}^m \exp^{\log q_{ij}+\log b_j(o_{t+1})+\log \beta_{t+1}(j)}
    \quad t=T-1,\ldots,1, 
    \quad \forall i \in\K
    \label{eq: log inductive backward}
\end{equation}

As such, given equation \ref{eq: log inductive backward} and \ref{eq: log inductive step forward prob} it is possible to derive the probability of being in a specific state at time $t$. Furthermore, in order to avoid the issues related to underflow, one needs to transform the posterior probabilities as well. This can be completed as shown in equation \ref{eq: base log posterior} and \ref{eq: inductive log posterior}.

\begin{equation}
    \log \upsilon_1(i) = \log P(s_t=i | o_1) = \log \delta_1 + \log b_j(o_{1}) 
    \label{eq: base log posterior}
\end{equation}

\begin{equation}
    \max_{i} \Big[\upsilon_{t-1}(i) + q_{ij}\Big] + \log b_j(o_t),
    \quad t=2,\ldots,T,
    \quad i \in\K
    \label{eq: inductive log posterior}
\end{equation}

As such, equation \ref{eq: base log posterior} and \ref{eq: inductive log posterior} captures the optimal state for each time step $t$, while ensuring that all the inputs are log-scaled, thereby avoiding underflow. It should be noted that similar to the procedure presented in equation \ref{eq: inductive viterbi 2}, the result produced from equation \ref{eq: inductive log posterior} for each time step $t$ should be stored in a variable such that the solutions can be back-traced in the decoding procedure. Lastly, in order to fully implement and capitalize on the log-scaling, it is necessary to adjust the re-estimation procedure presented in the EM-algorithm which were introduced by equation \ref{eq: gamma} and \ref{eq: xi}. The associated log transformation can be conducted as per below.

\begin{equation}
    \log\gamma_i(t) = \log Pr(s_t=i|O) =
    \frac{\log\alpha_i(t) + \log\beta_i(t)}{\log\Big[\sum_{i=1}^K \exp^{\log a_t(i)+\log q_{ij}}\Big]}  
    \label{eq: log gamma}
\end{equation}

\begin{equation}
    \log\xi_{ij}(t) = \log Pr(s_{t-1}=i, s_t=j|O) =
    \frac{\log\alpha_{t-1}(i) + \log q_{ij} + \log b_j(o_{t+1}) + \log \beta_{t+1}(j)}{\log\Big[\sum_{i=1}^K \exp^{\log a_t(i)+\log q_{ij}}\Big]}
    \label{eq: log xi}
\end{equation}

As such, it should be evident that by relying on the methodology introduced above it becomes feasible to implement the algorithms comprising the HMM, particularly in situation and environments where both $T$ and $m$ are large. The following section will introduce another approach to estimating HMMs which is known as jump estimation.

\subsubsection{Pseudo-code showcasing the implementation of log-scalling}
In order to provide the reader with an intuitive overview and approach to repeat the procedures outlined through scaling the likelihood computation and the associated parameters, this section will focus on providing pseudo-code for the computations. 

The section will be initialized with a walkthrough of the pseudo-code underlying the transformation of $\alpha_t(i)$ after-which the remaining procedures will follow. The reader should note that the provided pseudo-code is wrapped with Cython, which speeds of the computations with a factor 15-20x.

\begin{algorithm}[H]
\textbf{First iteration}: for t = 0 in range(n\_states): \;
\Indp
log\_alphas[0,i] = log\_startprob[i] + log\_emission\_proba[0,i] \;
\BlankLine
\Indm
\textbf{Inductive step}: for t = 1,...,n\_samples - 1 $\forall j \in \K$ \;
\Indp 
for t in range(1, n\_samples): \;
\Indp
for j in range(n\_states): \;
\Indp
for i in range (n\_states): \;
\BlankLine
\Indp
alpha\_temp[i] = log\_alphas[t-1, i] + log\_tpm[i, j] \;
\Blankline
\Indm \Indm
log\_alphas[t, j] = logsumexp\_cython(alpha\_temp, n\_states) + log\_emission\_proba[t, j] \;
\caption{Compute the log-scaled forward probabilities for all states $S_i$ and observations $O_t$}
\label{algo: log_scaled_forward}
\end{algorithm}

The workings and mechanics of the log forward probabilities derived from algorithm \ref{algo: log_scaled_forward} are closely interconnected with the backward probabilities. In the log space Closely connected to the mechanics and workings of the forward algorithm, the objective of the log scaled backward algorithm is to derive the log scaled backward probabilities.  

\begin{algorithm}[H]
\textbf{First iteration}: for t = n\_samples - 1 \;
for i in range(n\_states): \;
\Indp
log\_betas[n\_samples - 1, i] = 0 \;
\BlankLine
\Indm
\textbf{Inductive step}: for t = n \_samples - 2,...0 $ \forall j \in \K $ \;
for t in range(n\_samples - 2, -1, -1): \;
\Indp 
for i in range(n\_states): \;
\Indp
for j in range(n\_states): \;
\Indp
beta\_temp[j] = (log\_q[i, j] + log\_$b_j$($o_{t+1}$ + log\_betas[t+1, j] \;
\BlankLine
\Indm
log\_betas[t, i] = logsumexp\_cython(beta\_temp, n\_states) \;
\caption{Compute the log-scaled backward probabilities for all states $S_i$ and observations $O_t$}
\label{algo: log_scaled_backward}
\end{algorithm}

In addition to the log scaled forward and backward probabilities, a crucial element is the log scaled posterior probabilities. A pseudo-code overview of the implementation is provided in algorithm \ref{algo: log_scaled_posterior}.

\begin{algorithm}[H]
for i in range(n\_states): \;
\Indp 
log\_posterior[0, i] = log\_startprob[i] + log\_emission\_proba[0, i] \;
\Indm
for t in range(1, n\_samples): \;
\Indp
for i in range(n\_states): \;
\Indp
for j in range(n\_states): \;
\Indp
temporary\_calculation = (log\_tpm[j, i] + log\_posterior[t - 1, j]) \;
\Indm
log\_posteriors[t, i] = max(temporary\_calculation) + log\_emission\_proba[t, i] \;
\BlankLine
\caption{Compute the posterior probabilities for all states $\K$ at each time stept $t$}
\label{algo: log_scaled_posterior}
\end{algorithm}

Lastly, in order to fully implement and capitalize on the log-scaling, it is necessary to adjust the re-estimation procedure presented in the EM-algorithm which were introduced by equation \ref{eq: gamma} and \ref{eq: xi}. As such, algorithm \ref{algo: log_scaled_gamma} and \ref{algo: log_scaled_xi} will provide an overview of the log scaling for $\gamma_i(t)$ and $\xi_{ij}(t)$, since these are instrumental for the workings of the Baum-Welch algorithm underlying the MLE procedure.

\begin{algorithm}[H]
def\_compute\_gamma(self, log\_alphas, log\_betas): \;
\Indp 
log\_gamma = log\_alphas + log\_betas \;
normalizer = logsumexp(log\_gamma, axis = 1, keepdims = True) \;
log\_gamma -= normalizer \;
\Blankline
return log\_gamma
\caption{Compute the log scaled gamma probabilities, $\gamma_i(t)$ i.e. the probability of seeing state $i$ at time $t$ given the entire observation sequence $O^T$}
\label{algo: log_scaled_gamma}
\end{algorithm}

\begin{algorithm}[H]
def\_compute\_xi(self, log\_alphas, log\_betas): \;
\Indp 
log\_xi = np.zeros(shape=(len(log\_alphas)-1, self.n\_states, self.n\_states) \;
with np.errstate(divide = 'ignore'): \;
\Indp 
log\_tpm = np.log(self.tpm) \;
\Blankline
\Indm
for i in range(self.n\_states): \;
\Indp
for j in range(self.n\_states): \;
\Indp
log\_xi[:, i, j] = log\_tpm[i, j] + log\_alphas[:-1, i] + log\_betas[1:, j] + self.log\_emission\_probs[1:, j] \;
\Indm
\Indm
normalizer = logsumexp(log\_xi, axis = (1,2)) \;
\Blankline
log\_xi = log\_xi - normalizer[:, np.newaxis, np.newaxis] \;
return log\_xi
\caption{Compute the log scaled xi, $\xi_{ij}(t)$ i.e. the expected number of transitions from state $i$ to $j$, $P(s_{t-1} = j, s_t = i |O^T)$}
\label{algo: log_scaled_xi}
\end{algorithm}

As such, the introduced algorithms serve as a stylized work-sheet of how the code for the log scaling procedure has been implemented. It should be clear that the code is by no means collectively exhaustive as several aspects have been left out, for instance the updating of model parameters for each iteration. For a full overview of the code the authors refer to appendix XXX and github YYY. The following section labelled \ref{subsection: Jump theory} will introduce an alternative procedure to estimate HMM parameters as oppossed to the intuitive and standardized MLE approach.

\subsection{Jump estimation}
\label{subsection: Jump theory}

Apart from maximum likelihood estimation, other ways to estimate HMM's include jump estimation as shown by Bemporad et al. (2018). Most of the notation in this section follows that of the MLE section, though with minor differences. Following the model proposed by Bemporad et al. (2018), we define the following jump model; When given data $o_t\in \mathbb{R}^d$, $t=1,\ldots,T$, then, $o_t$ is a feature vector of order $d$, which for $d=1$ is univariate and for $d>1$ is multivariate. We are additionally given model parameters $\theta_{s_t}\in \mathbb{R}^d:s_t\in\K$, where the latent state variable $s_t$ determines which model parameter $\theta_{s_t}$ is active at time t. Then the loss function $\ell: O\times\mathbb{R}^d \rightarrow\mathbb{R}$, the regularizer term $r: \mathbb{R}^d \rightarrow \mathbb{R}$ and the state sequence loss $\mathcal{L}:\K^2 \rightarrow \mathbb{R}$ defines the jump model fitting objective
\begin{equation}
    J(O, \Theta, S) = \sum_{t=1}^T \ell(o_t,\theta_{s_t}) + \sum_{k=1}^K r(\theta_{s_t}) + \mathcal{L(S)}
\label{eq:jump_gen_objective}    
\end{equation}
which is minimized with respect to $\Theta=(\theta_1,\ldots,\theta_K)$ and the latent state sequence $\mathcal{S}=(s_1,\ldots,s_T)$. Thus, the general idea is that based on data O, we choose a number of K states and then minimize $J(O, \Theta, S)$ after which the distributional properties of each state $s$ will be fully explained by the model. Viewing the elements of $\Theta$ as cluster centers, the similarities to K-models models such as the K-means are clear. Bemporad et al. (2018) showed how HMM's can be seen as a special case of \cref{eq:jump_gen_objective} and for $\mathcal{L(S)} = \lambda\mathbf{1}_{\{ s_t\ne s_{t+1}\}}$, $\lambda \in \mathbb{R}$, and $r(\theta_{s_t})=0$, Nystrup et al. (2020) proposed the following alteration of \cref{eq:jump_gen_objective} for fitting fitting HMM's
\begin{equation}
    \sum_{t=1}^{T-1}[\ell(o_t, \theta_{s_t}) + \lambda\mathbf{1}_{\{ s_t\ne s_{t+1} \}}]
    + \ell(o_T, \theta_{s_T})
    ,\quad \lambda \geq 0
\label{eq:jump_objective}
\end{equation}

In \cref{eq:jump_objective}, $\lambda$ is a penalty that applies each time a state switch occur. It is essentially a hyper-parameter of the model, and potential tuning methods is shown in \cref{subsection: jump_penalizer}. Note that for $\lambda=0$, \cref{eq:jump_objective} reduces to a generalized K-means model (Lloyd, 1982), as the time ordering of observations become irrelevant. For $\lambda$ large enough, the model will converge towards a single state which will cover the entire dataset, thus $\lambda$ can be used to tune the persistence of states. The same loss as used by Nystrup et al. (2020) will be used in this analysis, making the loss function equal to the squared L2 norm\footnote{The squared L2 norm is defined as $\|\mathbf{x}\|_2^2 = (\sqrt{\sum_i x_i^2})^2 = \sum_i x_i^2 = \mathbf{x'x} $} $\ell(o_t, \theta_{s_t}) = \| o_t - \theta_{s_t} \|_2^2$, which is equivalent to the loss in K-means clustering.

\cref{algo:jump_fit} describes our estimation approach to fitting jump models. From a univariate time series $O$, a number of standardized features $Z$ are constructed and inputted to the algorithm along with the number of latent states K and an initial guess of the state sequence $\mathcal{S}$. Then, the algorithm iterates between fitting the model that minimizes the loss function for a given state sequence, and finding the state sequence that minimize the objective given in \cref{eq:jump_objective}. The algorithm is terminated once the state sequence doesn't change from one iteration to the next, or when the change in the objective function is below some tolerance level. Upon termination, transition probabilities are found by counting the number of state transitions and sojourns from the final state sequence $\mathcal{S}^i$. The distributional properties of the HMM is estimated from the sample mean and sample variance in each state.

\begin{algorithm}[H]
\KwInput{Time series O, number of latent states K and initial state sequence $\mathcal{S}^0=(s_0^0,\ldots,s_T^0)$.}
\BlankLine

1. Construct a set of standardized features Z from O.\;
2. Iterate for $i=1,\ldots$ until $\S^i=\S^{i-1}$ \;
\Indp
(a) $\Theta^i = \argmin_{\Theta} \sum_{t=1}^T \ell(z_t, \theta_{s_t^{i-1}})$ (model fitting)\;
(b) $\S^i = \argmin_s \sum_{t=1}^{T-1} [\ell(o_t, \theta_{s_t}) + \lambda\mathbf{1}_{\{ s_t\ne s_{t+1} \}}]
    + \ell(o_T, \theta_{s_T})$ (state sequence fitting)\;
\Indm
3. Compute transition probabilities and distributional properties in each state.\;
\BlankLine
\KwOutput{HMM parameters and prediction of latent states.}

\caption{Jump estimation of HMM (Nystrup et al., 2020)}
\label{algo:jump_fit}
\end{algorithm}

The objective function in \cref{eq:jump_objective} is guaranteed to stay the same or decrease for every iteration in \cref{algo:jump_fit}, yet just as the EM algorithm from previous sections, \cref{algo:jump_fit} is sensitive towards initial starting values and may converge towards local minima. As a result, we run \cref{algo:jump_fit} from 10 different starting values generated using K-means++ (Arthur \& Vassilvitskii, 2007) the same way as proposed by Nystrup et al. (2020).

In part 1 of \cref{algo:jump_fit}, we construct a number of standardized features from the observed data $O$. The point of this exercise is that the inclusion of time series features might reveal additional information which we can use in making the model better respond to state switches. Zheng et al. (2019) originally introduced a feature set, with the purpose of better characterising their conditional distributions in a spectral clustering estimator for HMMs. We will build on these features, however, we will exclude all forward looking variables. Using windows length of of $l=6$ and $l=14$, the features are summarized in \cref{algo:jump_features}.

\begin{algorithm}[H]

\KwInput{Time series O, and window length $l$}
\BlankLine

1. Observation: $o_t$\;
2. Left absolute change: $|o_t-o_{t-1}|$ \;
3. Previous left absoulte change: $|o_{t-1}-o_{t-2}|$ \;
4. Left local mean: $mean(o_{t-l},\ldots,o_{t})$ \;
5. Left local std; $std(o_{t-l,\ldots, o_{t}})$ \;
6. Left local median: $median(o_{t-l},\ldots,o_{t})$ \;
7. Min/Max difference: $max(o_{t-l},\ldots,o_{t}) - min(o_{t-l},\ldots,o_{t})$ \;
\BlankLine


\KwOutput{Feature set Z, where each each feature $z_i$ is standardized as $z_i = \frac{\bar x-x_i}{\sigma}$}

\caption{Features used in Jump estimation of HMM's}
\label{algo:jump_features}
\end{algorithm}

Part 2.a in \cref{algo:jump_fit} can be easily minimized analytically when the loss has strictly convex forms such as the squared L2 norm used here, thus guaranteeing the solution to be the global minimum. Taking the partial derivative of $\sum_{t=1}^T \ell(z_t, \theta_{s_t^{i-1}})$ with respect to $\theta_j$ yields
\begin{equation}
   \frac{\delta \sum_{t=1}^{T} ((z_t-\theta_{s_t})'(z_t-\theta_{s_t}) }{\delta \theta_j}
   = \frac{\delta \sum_{t=1}^T (z_t'z_t - 2z_t'\theta_{st}+ \theta_{s_t}'\theta_{s_t} )}{\delta  \theta_j}
   = \sum_{t\forall s_t=j} ( 2\theta_{s_t}' - 2z_t' )
\label{eq:jump_derivative}
\end{equation}
Thus, the partial derivative for $\theta_j$ is the sum over all time steps t where the state $s_t=j$ of the matrix $(\theta_{st}'-2z_t')$. Setting \cref{eq:jump_derivative} equal to zero and solving for $\theta_j$ yields the solution: $\theta_j = \frac{1}{N_j} \sum_{t:s_t=j} z_t $, where $N_j$ is the number of elements in $\mathcal{S}$ that equals $j$. The solution in each state is thus simply a vector of averages from each times series features in $Z$.

Item 2.b in \cref{algo:jump_fit} is minimized recursively in a similar way to the viterbi algorithm explained eariler, except the time order of operations are reversed. Following the method outlined by Nystrup et al. (2020), $\S^i$ is found by computing the least costly state path $s_t$ through T data points. Define
\begin{align}
    \upsilon_T(s) &= \ell(Z_T, \theta_s) \\
    \upsilon_t(s) &= \ell(Z_T, \theta_s) + \min_j[\upsilon_{t+1}(j) + \lambda\mathbf{1}_{\{ i\ne j \}}],
    \quad t=T-1,\ldots,1
\label{eq:jump_cost_path}
\end{align}
Then, the least costly state sequence is found by
\begin{align}
    s_1 &= \argmin_j \upsilon_t(j) \\
    s_t &= \argmin_j[\upsilon_t(j) + \lambda\mathbf{1}_{\{ s_{t-1 \ne j} \}}], \quad t=2,\ldots,T
    \label{eq:jump_state_seq_path}
\end{align}
$\upsilon$ is thus defined as the least costly path from state $i$ to $j$ between two discrete time steps, as it merely determined by the tradeoff between the jump penalty $\lambda$ and the cost associated with moving to state j. Viewing all entries of $V$ one can easily see how this will represent costs associated with each specific path through T data points in $\upsilon$. \cref{eq:jump_state_seq_path} can therefore be interpreted as the least costly path through $V$. 

One of the main advantages to the jump model over maximum likelihood estimation is that the jump model fitting is entirely based on squared Euclidean distances and does not assume the states to follow any specific distributions. As such it is expected to be more robust when trained on data in which the underlying distribution(s) is unknown, which is often the case with financial data. This is an extremely important progression since misspecified distributions often are a huge error source in HMMs (and other probabilistic times series models), so the freedom from this assumption will potentially significantly improve the model fit. Additionally, as shown in \cref{algo:jump_features} it is very easy to add more features to the estimator, allowing the use of a variety of both endogenously and exogenously given variables. This makes it feasible to potentially test many different features and feature sets when training models - no small advantage when pursuing models with highly persistent states

Finally, the model is computationally lighter to fit and often only requires between 5-10 operations to converge whereas the MLE estimator often requires 50-100. In the following sections we will show how both models converge when trained in a number of simulation environments.