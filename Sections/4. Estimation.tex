\section{Estimation}
\label{section: estimation}

\subsection{Maximum likelihood estimation}
\textbf{Der er mange formler her der kan forsimples meget ned hvis de skrives som vektorer og matricer - eksempelvis likelihood funktionen \cref{eq: motive fb}} 

\textbf{Der er en del formler der bygger på at udlede algoritmer og metoder og forklarer hvorfor algoritmerne er som de er. De skal slettes, da det ikke er relevant for vores studie, i.e. vi undersøger ikke hvordan man optimerer estimeringen af HMM med MLE metoden. Målet er at definere for læseren den metode vi bruger til at regne modellerne og ikke mere. Efterfølgende kan man evt. snakke om fordele og ulemper ved de valgte metoder. Lige nu er afsnittet uproportinelt langt iforhold til jump delen.}

\textbf{Alle * som bruges til at gange elementer sammen med skal slettes. Enten skal der bruges $\cdot$ og ellers skal to elementer bare stå sammen, i.e $a\cdot b$ eller $ab$ er lige gode.}

The traditional way of estimating the parameters making up the HMM is through a methodology referred to as maximum likelihood estimation. The method can be split three key problems that needs to be solved:

    \quad 1. Determine the likelihood of which the model generated a sequence of  observations $O$. 
    
    \quad 2. Estimate the model parameters that maximize the likelihood of the observed data $\Theta$.

    \quad 3. Based on the observation sequence, infer the most likely sequence of states $\S$.
\label{subsection: MLE}
 
In order to solve the first problem, one needs to rely on a dynamic programming technique, known as the forward–backward algorithm. The second problem is solved by relying on the work by Baum et al. (1970) and Dempster et al. (1977), which involves utilizing the Baum–Welch algorithm. Finally, the third problem is solved by using the Viterbi algorithm. All the algorithms and their respective mathematics are outlined in the next three subsections. 
 
\subsubsection{The Forward-Backward Algorithm}
\label{Section: Forward backward}
In order to motivate the use of the forward-backward algorithm, one can imagine all possible combinations of state sequences i.e. $\S = s_1,\ldots,s_t$. It should be noted that the state sequence will always have the same length as the observation sequence $O$. As such, the relevant information to uncover is the probability of seeing a particular observation sequence, given a state sequence. After uncovering all the possible state sequences that can produce a particular observations sequence, the probabilities can be summed, and the answer simplifies to a likelihood. Mathematically, the likelihood ($L_T$) of the observations given the model parameters $\Theta$, can be written as expressed in equation \ref{eq: motive fb}.
\begin{equation}
    L_T = P(O_t|\Theta) = \sum_{S_1,\ldots,S_t} \delta_{s_1}*b_{s_1}*(o_1) * q_{s_1s_2} * b_{s_2}(o_2)*q_{s_2s_3} .....q_{s_t-1 s_t}*b_{s_t}*(o_t)
    \label{eq: motive fb}
\end{equation}

Therefore, it is evident from equation \ref{eq: motive fb} that if an observations sequence is assumed to consist of 10 observations, which can be generated by a variety of different state sequences, it is necessary to derive the probability of each potential state sequence that can generate the correct observation sequence and sum them up to find the probability of an observation sequence $O_t$ given a model $\Theta$. The problem with this approach is that when the number of states ($K$) and the number of observations ($T$) grows large the complexity increases, and this methodology becomes highly computationally expensive. The number of operations required to estimate equation \ref{eq: motive fb} simplifies to:
\begin{equation}
    (2T-1)*K^T+(K^T-1)
\end{equation}

As such, for an observation sequence with $T = 100$ and $K = 5$, following the procedure of equation \ref{eq: motive fb} would result in $10^{72}$ operations. When the number of observations and states grows larger, this becomes even more computationally heavy, hence a better methodology is needed.  
In order to reduce the computational complexity and solve the problem of deriving the probability of a particular observation sequence, the forward–backward algorithm is used. The results of the forward-backward algorithm will also be used in the subsequent Baum-Welch algorithm. Define the \textit{ith} element of the vector $\mathbf{\alpha_t(i)}$ as the forward probability.
\begin{align}
    \alpha_t(i) = P(o_1,\ldots,o_t, s_t = i | \Theta)
    \label{eq: forward prob}
\end{align}

As such, it is evident from equation \ref{eq: forward prob} that the forward trellis can be defined as the probability of seeing observation $o_1 $ to $o_t$ while ending up at state $i$ at time $t$. Therefore, the forward probability $\alpha_t(i)$ will be derived for each possible state at each time $t$, thereby implying that the forward trellis will be stored in a matrix of the size $\K$ x $T$. In addition, the derivation of the forward probability can be separated in a base, inductive and termination step as depicted by equation \ref{eq: base case forward prob}, \ref{eq: inductive step forward prob} and \ref{eq: termination step}.

\begin{equation}
    \alpha_1(i) = \delta_i*b_i(o_1),
    \quad  \forall i \in \K
    \label{eq: base case forward prob}
\end{equation}
\begin{equation}
    \alpha_{t+1}(j) = \Big[\sum_{i=1}^K \alpha_t(i)*q_{ij}\Big] * b_j(o_{t+1}),
    \quad 1\leq t \leq T,
     \forall i,j \in \K
    \label{eq: inductive step forward prob}
\end{equation}
\begin{equation}
    L_T = P(O|\Theta) = \sum_{i=1}^K a_T(i)
    \label{eq: termination step}
\end{equation}

The base case represents the probability of starting in state $i$ while observing $o_1$. After this initial calculation, which is done for all states, the inductive step is initialized. The inductive step derives the joint probability of seeing state $j$ at $t+1$ and observing the sequence $o_1,\ldots,o_t$. As such, the forward probabilities that were derived one time step before is stored in the aforementioned matrix, and each of these stored values are then multiplied with their transition probability of jumping to state $j$ at time $t+1$. Taking the sum of all the possible paths and multiplying with the emission probability of seeing $o_{t+1}$ yields $\alpha_{t+1}(j)$. Therefore, the forward probabilities are inductively rolled forward until termination is reached at $T$. At termination the forward probabilities are summed across all states and a single value is reached which entails the probability of observing a particular observation sequence based on a given set of model parameters $\Theta$.

The method depicted through equation \ref{eq: base case forward prob}, \ref{eq: inductive step forward prob} and \ref{eq: termination step} provides an intuitive and inductive framework as opposed to the more comprehensive methodology introduced by equation \ref{eq: motive fb}. Furthermore, by introducing the forward probability framework, the computational complexity reduces to $O(K^2T)$. This reduction in mathematical operations makes it appropriate to implement the algorithm in an operationally efficient manner, hence the methodology could be applied by institutional investors and fund managers. 

Although the forward probability is all one needs to solve the first problem in order to determine the probability of seeing an observation sequence $O$ given a model $\Theta$, the same methodology will be conducted backward in order to solve additional problems related to the Baum-Welch algorithm. The backward probability, denoted $\beta_t(i)$ is derived in a similar fashion to the forward algorithm defined in equation \ref{eq: forward prob}.

\begin{equation}
    \beta_t(i) = P(o_{t+1},\ldots,o_T | s_t = i, \Theta) 
\end{equation}

The primary difference between the forward and backward probabilities is that instead of looking at an observation sequence from the past up to $t$, the backward probability determines the probability of seeing an observation sequence from $t+1$ to $T$, given that the model is in state $i$ at time $t$. Similar to the forward probability one can separate the backward probabilities into a base and inductive step. 
\begin{equation}
    \beta_T(i) = 1 % 1\leq i \leq m%
    \label{eq: base backward}
\end{equation}
\begin{equation}
   \beta_t(i) = \sum_{j=1}^K q_{ij}*b_j(o_{t+1})*\beta_{t+1}(j),
   \quad t = T-1,\ldots,1,
   \quad \forall i,j \in\K
    \label{eq: Inductive backward}
\end{equation}

It is evident from equation \ref{eq: base backward} and \ref{eq: Inductive backward} that the backward probabilities are derived by summing over all of the states that the model might end up at, while taking into account the transition and emission probabilities as well as the $\beta$ at time $t+1$.

From the forward and backward probabilities it follows that the probability of observing a particular sequence $O$ and being in state $i$ at time $t$ can be found as
\begin{align}
    Pr(o_1,\ldots,o_t, s_t = i | \Theta) * 
    Pr(o_{t+1},\ldots,o_T | s_t = i, \Theta)
     &=\alpha_t(i) * \beta_t(i) \\
     &= Pr(O, s_t = i |\Theta)  \notag
\end{align}

As such, the likelihood of the observed data $O$ can be evaluated by marginalizing over all $K$ states:
\begin{equation}
    Pr(O | \Theta) = \sum_{i=1}^K \alpha_t(i) * \beta_t(i)
    \label{eq: solution problem 1}
\end{equation}

Given these properties, one can see that equation \ref{eq: termination step} and \ref{eq: solution problem 1} provide identical results when $t=T$. Conclusively, it should be noted that it is necessary to scale the probabilities to avoid numerical underflow, as described in Zucchini \& MacDonald (2009) when  implementing the forward-backward algorithm. This will be covered in section \ref{subsection: log scalling underflow}. 

\subsubsection{The Baum-Welch Algorithm}
There are two popular methodologies to maximizing the likelihood which involves the direct numerical maximization and the Baum-Welch algorithm, which is a unique case of the more general expectation-maximization (EM) algorithm developed by Baum et al. (1970). Of the two, the -Welch algorithm is often preferred due to its larger robustness to initial starting values (Nystrup, 2014). The initial qualified estimate of the starting values underlying the model $\Theta(Q,B,\delta)$ are crucial since, despite the fact that the likelihood is guaranteed to increase or remain the same, for each iteration of the EM algorithm, the convergence towards the global maximum might be slow, or worse, it might get stuck at a local maximum. In order to circumvent this, the paper relies on several runs of the algorithm with different starting values, thereby making it possible to uncover any significant deviations in the maximum likelihood. 

The EM algorithm maximizes the logarithm of the complete-data likelihood (CDLL), which includes the log likelihood of the observations $O$ and the hidden states $\S$. This can be written as

\begin{equation}
    \log(Pr(O, \S) = \log(\delta_{s_1} * \Pi_{t=2}^T q_{s_{t-1},s_t} * \Pi_{t=1}^T b_{s_t}(o_t)) 
\end{equation}
Alternatively, it can be expressed as:
\begin{equation}
    \log(Pr(O,\S) = \log(\delta_{s_1}) + \sum_{t=2}^T \log(q_{s_{t-1},s_t})+\sum_{t=1}^T log(b_{st}(o_t)))
    \label{eq: alternative_log_likelihood}
\end{equation}

In order to simplify the operational nature of the maximum likelihood function, the following binary random variables are introduced,
$$
\gamma_i(t) = 1 \ \text{if and only if}\ s_t = i
$$
and
$$
\xi_{ij}(t) = 1 \ \text{if and only if}\ s_{t-1}=i\ \text{and}\ s_t = j
$$
As such if no state transition occurs $\gamma_i(t)$ is activated and if a state transition occurs $\xi_{ij}(t)$ is activated. The CDLL can now be written as per equation \ref{eq: CDLL}.

\begin{equation}
    \log(Pr(O, \S)) = \sum_{i=1}^K \gamma_i(1) \log\delta_i + \sum_{i=1}^K \sum_{j=1}^K \Big(\sum_{t=2}^T \xi_{ij}(t)\Big) \log q_{ij}+ \sum_{i=1}^K \sum_{t=1}^T \gamma_i(t) \log b_i(o_t) 
    \label{eq: CDLL}
\end{equation}

The idea underlying the methodology of the EM algorithm is to replace the quantities of $\gamma_i(t)$ and $\xi_{ij}(t)$ by their conditional expectations given an observation sequence $O$ as well as the current model parameters $\Theta(Q, B,\delta)$. This is referred to as the estimation step (E-step) (Zucchini \& MacDonald, 2009). As such, $\hat{\gamma_i(t)}$ and $\hat{\xi_{ij}(t)}$ are defined as per,

\begin{equation}
    \hat{\gamma_i(t)} = Pr(s_t=i | O) = \frac{\alpha_i(t)*\beta_i(t)}{\sum_{j=1}^K \alpha_t(j)\beta_t(j)} = \frac{\alpha_i(t)*\beta_i(t)}{P(O|\Theta)}
    \label{eq: gamma}
\end{equation}
and
\begin{align}
    \hat{\xi_{ij}(t)} = Pr(s_{t-1}=i, s_t=j|o^{(T)}) &= \label{eq: xi} \\ 
    \frac{\alpha_{t-1}(i)*q_{ij}b_j(o_{t+1}\beta_{t+1}(j)}
            {\sum_{i=1}^K\sum_{j=1}^K\alpha_{t-1}(i)*q_{ij}b_j(o_{t+1}\beta_{t+1}(j)}
    &= \frac{\alpha_{t-1}(i)*q_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{P(O|\Theta)}  \notag
\end{align}

As such, it is evident that $\gamma_i(t)$ encompass the probability of being in state $i$ at time $t$ knowing all the observations that has come before and all the observations that is to come, regardless of all the possible ways the model could have arrived at $t$ as well as all the possible ways the model can continue after $t$. Given this analogy one can think of $\gamma_i(t)$ as a bowtie that ties the forward and backward probabilities together for a given state $i$ at a given time $t$. 

Furthermore, $\xi_{ij}(t)$ captures the probability of being in state $i$ at time $t$ and then transitioning to state $j$ at time $t+1$, given all the past observations and all the observations to come. Therefore, the similarity to $\gamma_i(t)$ is evident, however, $\xi_{ij}(t)$ takes into account two time steps. As such, given the properties of equation \ref{eq: gamma} and \ref{eq: xi} it is clear that $\sum_{j=1}^K \xi_{ij}(t) = \gamma_i(t)$. 

Having replaced $\gamma_i(t)$ and $\xi_{ij}(t)$ by their conditional expectations $\hat{\gamma_i(t)}$ and $\hat{\xi_{ij}(t)}$ the CDLL from equation \ref{eq: CDLL} is maximized with respect to three sets of parameters which includes the initial distribution $\delta$, the transition probability matrix $Q$ as well as the parameters of the state dependent distributions captured by $B$. This process is referred to as the maximization step (M-step). 

As such, the mechanism of the EM algorithm is to repeat the E-step and M-step until one or several convergence criteria have been satisfied. The criteria could entail that the iteration proceeds until the resulting change in the log likelihood is below some threshold. As briefly mentioned, since the EM algorithm is sensitive to starting values there is no guarantee that the model parameters converge to a global maximum, hence several starting values are tested. The solution set to the parameters of the model is defined as per below. 
\begin{equation}
    \delta_i = \frac{\hat{\gamma_i(1)}}{\sum_{i=1}^K\hat{\gamma_i(1)}} = \hat{\gamma_i(1)}
\end{equation}
\begin{equation}
    q_{ij} = \frac{\sum_{t=1}^{T-1}\hat{\xi_{ij}(t)}}{\sum_{t=1}^{T-1}\hat{\gamma_i(t)}}
\end{equation}

Furthermore, maximization of the third term in equation \ref{eq: CDLL} may be easy or difficult depending on the nature of the state-dependent distributions (Zucchini, 2009). In some cases numerical maximization will be necessary, however, in the case of conditional univariate normal
distributions, the maximizing values of the state-dependent parameters are defined as:
\begin{align}
    \hat\mu_j &= \frac{\sum_{t=1}^T\hat\gamma_j(t)o_t}{\sum_{t=1}^T \hat\gamma_j(t)} \\
    \hat\sigma_j^2 &= \frac{\sum_{t=1}^T\hat\gamma_j(t)(o_t-\hat\mu_j)^2}{\sum_{t=1}^T \hat\gamma_j(t)}
\end{align}

In order to utilize this on a large scale, for instance at an asset management firm or hedge fund, one must carefully consider the speed of convergence to the true model parameters. One way to increase the speed of convergence is through a hybrid algorithm that starts with the EM algorithm and switches to a Newton-type algorithm when a certain stopping criterion is fulfilled (Bulla \& Berzel, 2008). It should be clear by now that the likelihood of an HMM is a complicated function, which includes many model parameters, and as such has several local maxima. The goal of the algorithm is to find the model parameters that maximizes the likelihood. 

Furthermore, the EM algorithm is circular since it initializes with a qualified estimate of the model parameters $\Theta(Q, B,\delta)$. The forward-backward algorithm then derives $\alpha_t(i), \beta_t(i), \gamma_i(t)$ and $\xi_{ij}(t)$. This is the E-step. These derived parameters are then used to update the model parameters by maximizaing the CDLL log-likelihood function. This completes one iteration of the EM algorithm and the model starts over until it converges to a local or preferably global maximum. Alternative methodologies for maximizing the likelihood function do exist. For instance, a Bayesian approach based on Markov chain Monte Carlo (MCMC) sampling can be used (Frühwirth-Schnatter, 2006). In addition, it follows that Rydén (2008) compared the EM and MCMC approaches and found that MCMC can be advantageous for interval estimation and inferential problems, yet the EM serves as a simpler and faster way of obtaining point estimates. As such, the forward-backward and Baum-Welch (EM) algorithm have solved the first two problems, hence the only aspect left is to infer the most likely sequence of states. This is done through the Viterbi algorithm. 
 
\subsubsection{Decoding: The Viterbi Algorithm}
\label{subsection: Decoding}

\textbf{Meget langt afsnit syntes jeg. Se eksmepelvis ved Jump modellen hvor en nærmest identisk algoritme defineres på ca. 1/3 af en side. Der er igen en del udledning af hvorfor Viterbi algoritmen er efficient i forhold til brute-force maximering, men igen syntes jeg ikke det er relevant at have med da det ikke er noget vi reelt set undersøger eller bruger i resten af opgaven. Derudover er der fejl i matematikken - se nedenstående kommentarer}

\textbf{Hvorfor skal vi bruge så mangle variable btw? Brugen af både $\psi$ og $s_t$ virker ligegyldig da man reelt kunne skrive $\psi$ som sin statssekvens og dermed fjerne $\psi$ helt.}

Inferring the most likely sequence of states, which is the third problem of mle procedure, is referred to as decoding. This is a crucial aspect as a particular observation sequence can produce a variety of different state sequences since several conditional distribution can produce the same observations. Furthermore, decoding becomes important since the parameters must be optimized not just for the most probable sequence of states but the sequence of states must also be viable. For instance if the model suggests a jump from state $i$ to state $j$ at time $t$ to $t+1$, but due to constraints, such as a transition probability matrix of 0\%, the suggested transition may not viable in reality, hence the model must select the second most probable state transition accordingly. 

In addition, decoding is essentially the information that makes it possible to construct a trading / investment strategy based on a HMM, since an investor needs to know the states of the economy to allocate capital in an efficient manner. This means that a HMM, without a decoding procedure, becomes rather useless from an investment perspective. Decoding can be done either locally, by determining the most likely state at each time $t$ or globally by determining the most likely sequence of states (Zucchini \& MacDonald, 2009). The issue with local decoding is that it deduces the state sequence at each time instead of doing so across the entire time horizon. This can lead to suggested state sequences that are not viable, hence global decoding is preferred. As such, the following property can be defined.

\begin{equation}
    \argmax_{\S}Pr(\S|O, \Theta)
    \label{eq: global decoding}
\end{equation}

Maximizing equation \ref{eq: global decoding} over all possible state sequences by brute force involves $O(KT)$ operations and thus proves inefficient on large scale evaluations. Rather, the Viterbi algorithm can be used to compute
the most likely sequence of states in an efficient manner. 

The best way to comprehend the Viterbi algorithm is to realize that at $t+1$ the algorithm has to figure out the state at time $t$ that maximizes the probability of making a transition to a given state $i$ while observing a specific observation at time $t+1$. Once this is done, the state from time $t$ is stored. Now the algorithm moves on to time $t+2$ and find the state that maximizes a move from time $t+1$ to $t+2$ while observing a specific observation at time $t+2$. Then the state at $t+1$ that maximizes this is stored and so forth. Once the algorithm terminates at $T$ it will contain a map of valid states that can be followed and interpreted as a sequence. In order to implement the outlined reasoning some additional variables are introduced.

\textbf{Hvad er $\psi$? Definer hvad det er lig med.}
\begin{equation}
    \upsilon_t(i) =  \max_{\S}Pr(\S|O, \Theta)
\end{equation}
and 
$$
{\psi_t(i)}
$$

As such, $\upsilon_t(i)$ finds the path with the highest probability that accounts for the first $t$ observations and ends up at state $i$ at time $t$ and $\psi_t(i)$ serves as a storing variable that keeps track of optimal state at each time step $t$. Similar to the previously described algorithms, the Viterbi algorithm can be separated into a base, inductive and termination step.

\textbf{Der er fejl i de her formler. $\psi_1(i)$ er ikke defineret som 0, og det er heller ike sådan vi regner det.}
\begin{equation}
    \upsilon_1(i) = \delta_i*b_i(o_1)
    \label{eq: viterbi start 1}
\end{equation}
\begin{equation}
     \psi_1(i) = 0 
\end{equation}
\begin{equation}
    \upsilon_t(j) = \max_{i}\Big[\upsilon_{t-1}(i)q_{ij}\Big] *b_j(o_t),
    \quad t=2,\ldots,T , \quad i,j \in \K 
    \label{eq: inductive viterbi}
\end{equation}
\begin{equation}
    \psi_t(j) = {\argmax_{i} \Big[\upsilon_{t-1}(i)q_{ij}\Big]},
    \quad {t= T-1,\ldots,1}, \quad i,j\in\K
    \label{eq: inductive viterbi 2}
\end{equation}

Naturally, the best initial state is the one that maximizes the probability of observing a particular observation $o_1$ as expressed in equation \ref{eq: viterbi start 1}. Furthermore, it is obvious that the storing variable $\psi_t(i)$ is 0 at initialization since no optimal state has been discovered.

The first term in equation \ref{eq: inductive viterbi} is the state that maximized $\upsilon$ at the previous time step. In this case the state is set to $i$ but the algorithm scans over all the potential states that the model could have come from. After doing so the algorithm considers all the ways of getting from state  $i$ to $j$ which is the transition probability, which is then multiplied with the probability of observing $o_{t+1}$. Therefore, the max function of equation \ref{eq: inductive viterbi} is going to find the state from the previous time period that maximizes the probability of moving to the current state.  

The termination step of the algorithm is presented in equation \ref{eq: viterbi termination 1} and \ref{eq: viterbi termination 2}

\textbf{Jeg forstår ikke det her. Så den endelige stat er lig med indekspositionen af start-sandsynlighederne? Det er ikke sådan vi regner det her. Skal rettes.}
\begin{equation}
    \upsilon^* = \max_{i}\Big[\delta_T(i)\Big],
    \quad i \in\K
    \label{eq: viterbi termination 1}
\end{equation}
 \begin{equation}
     s_T^*= \argmax_{i}\Big[\delta_T(i)\Big],
     \quad i \in\K
     \label{eq: viterbi termination 2}
 \end{equation}
Where $\upsilon^*$ expresses the highest probability for the valid sequence of states given an observation sequence. As such, knowing $\upsilon^*$ it is possible to iterate backwards through the process as
\begin{equation}
    s_t^* = \psi_{t+1}*(s_{t+1}^*)
\end{equation}

Comparing equation \ref{eq: inductive viterbi} with equation \ref{eq: inductive step forward prob} it is evident that the dynamic programming technique for the Viterbi algorithm is the same as in the forward–backward
algorithm. The only difference is the substitution of the sum with a maximization. From a computational efficiency perspective the Viterbi algorithm remains at $O(K^2T)$ operations, thereby making it feasible for full-scale production applications (Dittmer 2008).

\subsection{Logarithmic scaling to handle numerical underflow}
\label{subsection: log scalling underflow}

The theory above have outlined the mathematics comprising HMMs, however, as briefly mentioned, a crucial aspect regarding their implementation is dealing with the underflow and/or overflow produced by the algorithms. The issues regarding underflow arises when $T$ gets large since the algorithms will begin multiplying together smaller and smaller numbers. Overflow is the inverse to underflow and it occurs when continuously larger numbers are multiplied together. As such, underflow and overflow proposes a challenge to the implementation of HMMs as Python or other libraries will generate an error or simply interrupt the execution of the code. The challenges associated with underflow and overflow is an area that is widely overlooked in the literature, despite the fact that it is of crucial importance in terms of utilizing HMMs on a full-scale operational level. As such, this section aims at proposing a solution towards the challenges associated with underflow and overflow.

\subsubsection{Scaling the likelihood computation}
\textbf{Skriv om således den nødvendige matematik per. algoritme kommer lige før dennes pseudo-kode.}

For discrete state-dependent distributions, the element of the forward probabilities $\alpha_t(i)$, as seen in equation \ref{eq: inductive step forward prob}, becomes progressively smaller as $t$ increases because the forward probabilities are made up of products of probabilities, hence they will converge exponentially towards zero. The remedy needed to circumvent this issue is the same for overflow and underflow, hence the authors confine their attention to underflow.

A circumvention of the challenges related to underflow can be implemented by computing the logarithm of $L_T$ in equation \ref{eq: motive fb} which involves using a strategy of scaling the vector of forward and backward probabilities $\alpha_t(i)$ and $\beta_t(j)$. As such, a scaling of the vector of forward and backward probabilities can be conducted by log transforming the traditional formulas presented in section \ref{Section: Forward backward}. This methodology does not change the workings of the algorithms, however, it reduces the problems associated with underflow since the forward and backward probabilities no longer becomes excessively small when $t$ increases. Since the log-transformation will be rooted in the formulas presented in \ref{Section: Forward backward}, one can separate the transformations into a base, inductive and termination step.  

\begin{equation}
    \log\alpha_1(i) = \log\delta_i + \log b_i(o_1)
    \quad \forall i \in\K
    \label{eq: log base case forward prob}
\end{equation}

\begin{equation}
    \log\alpha_{t+1}(j) = \log\Big[\sum_{i=1}^K \exp^{\log a_t(i)+\log q_{ij}}\Big] + \log b_j(o_{t+1}),
    \quad t=2,\ldots,T-1, 
    \quad \forall j \in\K
    \label{eq: log inductive step forward prob}
\end{equation}

\begin{equation}
     L_T = \log P(O |\Theta) = \log\sum_{i=1}^K \exp^{\log\alpha_T(i)}
    \label{eq: log termination step}
\end{equation}

As should be evident by equation \ref{eq: log base case forward prob}, \ref{eq: log inductive step forward prob} and \ref{eq: log termination step} a large part of the transformation relies on the following mathematics $log(ab) = log(a) + log(b)$. Furthermore, the transformation, particularly for the inductive step in equation \ref{eq: log inductive step forward prob}, relies on the mathematics guiding the LogSumExp (LSE) function. The LSE is defined as $LSE(x_1,...,x_n) = log(exp^{(x_1)}+...+exp^{(x_n)})$.

Yet, the utilization of the LSE function, as recently defined, can bring additional problems related to underflow. Therefore, the following formula, which provides equivalent results as the provided LSE formula, must be used instead. The revised LSE formula is defined as $LSE(x_1,....,x_n) = x^* + log(exp(x_1-x^*)+....+exp(x_n-x^*))$ where $x^* = max(x_1,.....,x_n)$. Despite having more than one variable, the procedure remains the same, yet the $x^*$ will be selected from a range of multiple vectors. This mathematical trick is done in order to ensure that the number, that is subject to be log transformed, is above 0 since the natural logarithm is undefined for values below 0. By utilising these logarithmic formulas it is possible to conduct a similar transformation for the backward probabilities derived from equation \ref{eq: base backward} and \ref{eq: Inductive backward}. 

\begin{equation}
     \log\beta_T(i) = 0
    \label{eq: log base backward}
\end{equation}

\begin{equation}
    \beta_t(i) = \log\sum_{j=1}^m \exp^{\log q_{ij}+\log b_j(o_{t+1})+\log \beta_{t+1}(j)}
    \quad t=T-1,\ldots,1, 
    \quad \forall i \in\K
    \label{eq: log inductive backward}
\end{equation}

As such, given equation \ref{eq: log inductive step forward prob} and \ref{eq: log inductive backward} it is possible to derive the log scaled probability of being in a specific state at time $t$. Furthermore, in order to avoid the issues related to underflow, one needs to transform the posterior probabilities as well. This can be completed as shown in equation \ref{eq: base log posterior} and \ref{eq: inductive log posterior}.

\begin{equation}
    \log \upsilon_1(i) = \log P(s_t=i | o_1) = \log \delta_1 + \log b_j(o_{1}) 
    \label{eq: base log posterior}
\end{equation}

\begin{equation}
    \max_{i} \Big[\upsilon_{t-1}(i) + q_{ij}\Big] + \log b_j(o_t),
    \quad t=2,\ldots,T,
    \quad i \in\K
    \label{eq: inductive log posterior}
\end{equation}

As such, equation \ref{eq: base log posterior} and \ref{eq: inductive log posterior} captures the optimal state for each time step $t$, while ensuring that all the inputs are log scaled, thereby avoiding underflow. It should be noted that, similar to the procedure presented in equation \ref{eq: inductive viterbi 2}, the result produced from equation \ref{eq: inductive log posterior} should be stored in a variable such that the solutions can be back-traced in the decoding procedure. Lastly, in order to fully implement and capitalize on the log scaling, it is necessary to adjust the re-estimation procedure presented in the EM algorithm which was introduced by equation \ref{eq: gamma} and \ref{eq: xi}. The associated log transformation can be conducted as per below.

\begin{equation}
    \log\gamma_i(t) = \log Pr(s_t=i|O) =
    \frac{\log\alpha_i(t) + \log\beta_i(t)}{\log\Big[\sum_{i=1}^K \exp^{\log a_t(i)+\log q_{ij}}\Big]}  
    \label{eq: log gamma}
\end{equation}

\begin{equation}
    \log\xi_{ij}(t) = \log Pr(s_{t-1}=i, s_t=j|O) =
    \frac{\log\alpha_{t-1}(i) + \log q_{ij} + \log b_j(o_{t+1}) + \log \beta_{t+1}(j)}{\log\Big[\sum_{i=1}^K \exp^{\log a_t(i)+\log q_{ij}}\Big]}
    \label{eq: log xi}
\end{equation}

As such, it should be evident that by relying on the methodology introduced above it becomes feasible to implement the algorithms comprising the HMM, particularly in situation and environments where both $T$ and $K$ are large. The following section will introduce pseudo-code which showcases the implementation of the log scaling for HMMs. 

\subsubsection{Pseudo-code showcasing the implementation of log-scaling}
In order to provide the reader with an intuitive overview and approach to repeat the procedures associated with scaling the likelihood computation and the associated parameters, this section will focus on providing pseudo-code. As such, the section will be initialized with a walk-through of the pseudo-code underlying the transformation of $\alpha_t(i)$, after which the transformations of the remaining parameters will follow. The reader should note that the provided pseudo-code is wrapped with Cython, which speeds up the computations with a factor 15-20x.

\textbf{Der skal ikke stå $for all i \in \K$. Hvis man looper over stater hedder det "for $i=1,\ldots,K$".}

\begin{algorithm}[H]

\For{t = 1 all $i \in \K$}
    {$\log\alpha_{1}(i) = \log\delta_i + \log b_i(o_1) $}  \;
\For{t = 2,\ldots,T-1}{
    \For{all $j \in \K$}{
        \For{ all $i \in \K$}{
            temp $\alpha(i) = \log\alpha_{t-1}(i) + \log(q_{i, j})$}
        $\log\alpha_{t}(j)$ = LSE(temp $\alpha(i)$) + $\log b_j(o_t)$} \;   }
        
\caption{Compute the log scaled forward probabilities for all states $S$ and observations $O$}
\label{algo: log_scaled_forward}
\end{algorithm}

The workings and mechanics of the log forward probabilities, derived from algorithm \ref{algo: log_scaled_forward}, are closely interconnected with the backward probabilities. As such, the log scaling of the backward probabilities can be done in a similar fashion as the forward probabilities.   

\begin{algorithm}[H]
\For{t = T - 1 all $i \in \K$}
{$\log \beta_{T-1}(i) = 0$} \;
\For{t = 1\ldots T-2}{
    \For{all $i \in \K$}{
        \For{all $j \in \K$}{
            temp $\beta(j)$ = $\log q_{ij}$ + $\log b_j(o_t)$ + $\log \beta_{t+1}(j)$  }
        $\log \beta_t(i)$ = LSE(temp$\beta(j))$}}


\caption{Compute the log scaled backward probabilities for all states $S$ and observations $O$}
\label{algo: log_scaled_backward}
\end{algorithm}

In addition to the log scaled forward and backward probabilities, a crucial element is the log scaled posterior probabilities. A pseudo-code overview of the implementation is provided in algorithm \ref{algo: log_scaled_posterior}.


\begin{algorithm}[H]
\For{all $i \in \K$}
{$\log \upsilon_1(i) = \log \delta_i + \log b_i(o_1)$}\;

\For{t in all $\K$}{
    \For{all $i \in \K$}{
        \For{ all $j \in \K$}{
           temporary\_calculation = $\log q_{j,i} + \log \upsilon_{t-1}(j)$}
      $\log \upsilon_t(i)$ = max(temporary\_calculation) + $\log b_i(o_t)$} \;   }

\caption{Compute the log scaled posterior probabilities for all states $\K$ at each time step $t$}
\label{algo: log_scaled_posterior}
\end{algorithm}

Lastly, in order to fully implement and capitalize on the log scaling, it is necessary to adjust the re-estimation procedure presented in the EM algorithm through equation \ref{eq: gamma} and \ref{eq: xi}. As such, algorithm \ref{algo: log_scaled_gamma} and \ref{algo: log_scaled_xi} will provide an overview of the log scaling for $\gamma_i(t)$ and $\xi_{ij}(t)$, since these are instrumental for the workings of the Baum-Welch algorithm underlying the mle procedure.

\textbf{Vi skal ikke bruge funktioner i det her, udelukkende loops. Hvis man endelig skulle bruge en funktion så skal det være den LaTex tilbyder med dertilhørende formattering - dette er for python-agtigt.}

\begin{algorithm}[H]
def\_compute $\gamma_i(t)$(self, $\log\alpha_t(i), \log\beta_t(i)$): \;
\Indp 
$\log\gamma_i(t) = \log\alpha_t(i) + \log\beta_t(i)$ \;
normalizer = LSE($\log\gamma_i(t)$) \;
$\log\gamma_i(t)$ -= normalizer \;
\BlankLine
return $\log\gamma_i(t)$
\caption{Compute the log scaled gamma probabilities, $\gamma_i(t)$ i.e. the probability of seeing state $i$ at time $t$ given the entire observation sequence $O$}
\label{algo: log_scaled_gamma}
\end{algorithm}

\newpage

\textbf{Den her skal jeg nok tage når jeg lige får tid.}

\begin{algorithm}[H]
def\_compute $\xi$(self, $\log\alpha_t(i)$, $\log\beta_t(i)$): \;
\Indp 
$\log Q$ = np.log(self.tpm) \;
\BlankLine

\For{all $i \in \K$}{
    \For{all $j \in \K$}{
        $\log\xi_{ij}$ = $\log$ $q_{i,j} +$ $\log\alpha_{t}(i)$ + $\log\beta_t(j) + b_j(o_t)$}
    normalizer = LSE($\log\xi_{ij}(t)$) \;
    $\log\xi_{ij}$ = $\log\xi_{ij}$ - normalizer[:, np.newaxis, np.newaxis] \;
    return $\log\xi_{ij}$}

\caption{Compute the log scaled $\xi_{ij}(t)$ i.e. the expected number of transitions from state $i$ to $j$, $P(s_{t-1} = j, s_t = i |O)$}
\label{algo: log_scaled_xi}
\end{algorithm}

As such, the introduced algorithms serve as a stylized worksheet of how the code for the log scaling procedure has been implemented. It should be clear that the code is by no means collectively exhaustive as several aspects have been left out, for instance the updating of model parameters for each iteration. For a full overview of the code the authors refer to github YYY. This covers the workings and mathematics underlying the mle approach, however, section \ref{subsection: Jump theory} will introduce an alternative procedure to estimating HMM parameters. This approach is known as jump estimation.

\subsection{Jump estimation}
\label{subsection: Jump theory}
In a recent paper by Bemporad et al. (2018) it was shown that the parameters governing HMMs can be estimated by jump models. Most of the notation in this section follows that of the prior sections, although minor differences do exists. Following the model proposed by Bemporad et al. (2018), we define the following jump model. Given data $o_t\in \mathbb{R}^d$, $t=1,\ldots,T$, then $o_t$ is a feature vector of order $d$, which for $d=1$ is univariate and multivariate for $d>1$. Furthermore, define model parameters as $\theta_{s_t}\in \mathbb{R}^d:s_t\in\K$, the latent state variable $s_t$ determines which model parameter $\theta_{s_t}$ is active at time $t$. Then the loss function $\ell: O\times\mathbb{R}^d \rightarrow\mathbb{R}$, the regularizer term $r: \mathbb{R}^d \rightarrow \mathbb{R}$ and the state sequence loss $\mathcal{L}:\K^2 \rightarrow \mathbb{R}$ defines the jump model fitting objective
\begin{equation}
    J(O, \Theta, S) = \sum_{t=1}^T \ell(o_t,\theta_{s_t}) + \sum_{k=1}^K r(\theta_{s_t}) + \mathcal{L(S)}
\label{eq:jump_gen_objective}    
\end{equation}
which is minimized with respect to $\Theta=(\theta_1,\ldots,\theta_K)$ and the latent state sequence $\mathcal{S}=(s_1,\ldots,s_T)$. As such, the general idea is that based on a data sequence $O$, a number of $K$ states are chosen after which $J(O, \Theta, S)$ is minimized. By doing so, the distributional properties of each state $s$ will be fully explained by the model. By viewing each individual element $\theta$ in $\Theta$ as cluster centers, the similarities to K-models models such as the K-means are clear. Furthermore, Bemporad et al. (2018) showed how HMMs can be seen as a special case of \cref{eq:jump_gen_objective} and using the state sequence loss $\mathcal{L(S)} = \lambda\mathbf{1}_{\{ s_t\ne s_{t+1}\}}$, $\lambda \in \mathbb{R}$, and $r(\theta_{s_t})=0$, Nystrup et al. (2020) proposed the following alteration of \cref{eq:jump_gen_objective} for fitting fitting HMM's
\begin{equation}
    \sum_{t=1}^{T-1}[\ell(o_t, \theta_{s_t}) + \lambda\mathbf{1}_{\{ s_t\ne s_{t+1} \}}]
    + \ell(o_T, \theta_{s_T})
    ,\quad \lambda \geq 0
\label{eq:jump_objective}
\end{equation}

In \cref{eq:jump_objective}, $\lambda$ is a penalty that applies each time a state transition occurs. As such, it is essentially a hyper parameter of the model hence potential tuning methods are shown in \cref{subsection: jump_penalizer}. The reader should note that for $\lambda=0$, \cref{eq:jump_objective} reduces to a generalized K-means model as the time ordering of observations become irrelevant (Lloyd, 1982). When $\lambda$ is large enough, the model will converge towards a single state which will cover the entire dataset, and thus $\lambda$ can be used to tune the persistence of states. The same loss, as used by Bemporad et al. (2018) and Nystrup et al. (2020), will be used in this analysis, thereby making the loss function equal to the squared $\ell_2$ norm\footnote{The squared $\ell_2$ norm is defined as $\|\mathbf{x}\|_2^2 = (\sqrt{\sum_i x_i^2})^2 = \sum_i x_i^2 = \mathbf{x'x} $} $\ell(o_t, \theta_{s_t}) = \| o_t - \theta_{s_t} \|_2^2$, which is equivalent to the loss in K-means clustering.

\cref{algo:jump_fit} describes our estimation approach to fitting jump models. From a univariate time series $O$, a number of standardized features $Z$ are constructed and inputted to the algorithm along with the number of latent states $K$ and an initial guess of the state sequence $\mathcal{S}$. Then, the algorithm iterates between fitting the model that minimizes the loss function for a given state sequence, and finding the state sequence that minimize the objective given in \cref{eq:jump_objective}. The algorithm is terminated once the state sequence remains unchanged from one iteration to the next, or when the change in the objective function is below some tolerance level. Upon termination of the algorithm, transition probabilities are found by counting the number of state transitions and sojourns from the final state sequence $\mathcal{S}^i$. The distributional properties of the HMM is estimated from the sample mean and sample variance in each state.

\begin{algorithm}[H]
\KwInput{Time series O, number of latent states K and initial state sequence $\mathcal{S}^0=(s_0^0,\ldots,s_T^0)$.}
\BlankLine

1. Construct a set of standardized features Z from O.\;
2. Iterate for $i=1,\ldots$ until $\S^i=\S^{i-1}$ \;
\Indp
(a) $\Theta^i = \argmin_{\Theta} \sum_{t=1}^T \ell(z_t, \theta_{s_t^{i-1}})$ (model fitting)\;
(b) $\S^i = \argmin_s \sum_{t=1}^{T-1} [\ell(o_t, \theta_{s_t}) + \lambda\mathbf{1}_{\{ s_t\ne s_{t+1} \}}]
    + \ell(o_T, \theta_{s_T})$ (state sequence fitting)\;
\Indm
3. Compute transition probabilities and distributional properties in each state.\;
\BlankLine
\KwOutput{HMM parameters and prediction of latent states.}

\caption{Jump estimation of HMM (Nystrup et al., 2020)}
\label{algo:jump_fit}
\end{algorithm}

The objective function in \cref{eq:jump_objective} is guaranteed to stay the same or decrease for every iteration in \cref{algo:jump_fit}, however, just as the EM algorithm from the previous sections, \cref{algo:jump_fit} is sensitive towards initial starting values and may converge towards local minima. As a result, \cref{algo:jump_fit} is run for 10 different starting values of $\Theta$ and $\S$ generated using K-means++ (Arthur \& Vassilvitskii, 2007) which is identical to the proposed way by Nystrup et al. (2020).

In part 1 of \cref{algo:jump_fit}, a number of standardized features from the observed data $O$ are constructed. This is done since the inclusion of time series features might reveal additional information which can be used in making the model respond better to state switches. As such, Zheng et al. (2019) originally introduced a feature set with the purpose of better characterising conditional distributions in a spectral clustering estimator for HMMs. We will expand on these features, although as oppossed to Zheng et al. (2019) and Nystrup et al. (2020) we will omit the use of forward looking variables. The reason for this is that it introduces an obvious look-ahead bias, which renders the model useless in an out-of-sample setting. Feature selection will be explained in detail throughout \cref{subsection: jump_penalizer}.

Part 2.a in \cref{algo:jump_fit} can easily be minimized analytically when the loss has strictly convex forms such as the squared $\ell_2$ norm used here, thereby guaranteeing the solution to be the global minimum. Taking the partial derivative of $\sum_{t=1}^T \ell(z_t, \theta_{s_t^{i-1}})$ with respect to $\theta_j$ yields,
\begin{equation}
   \frac{\delta \sum_{t=1}^{T} ((z_t-\theta_{s_t})'(z_t-\theta_{s_t}) }{\delta \theta_j}
   = \frac{\delta \sum_{t=1}^T (z_t'z_t - 2z_t'\theta_{st}+ \theta_{s_t}'\theta_{s_t} )}{\delta  \theta_j}
   = \sum_{t\forall s_t=j} ( 2\theta_{s_t}' - 2z_t' )
\label{eq:jump_derivative}
\end{equation}
hence the partial derivative for $\theta_j$ is the sum over all time steps $t$ where the state $s_t=j$ of the vector $(\theta_{st}'-2z_t')$. Said differently, we first split the data into those observation related to state $j$, and then compute the sum over all those observations for the vector $(\theta_{st}'-2z_t')$. Ignoring the $t\forall s_t=j$ part of the sum in \cref{eq:jump_derivative}, the matrix calculus should be clear. The sum $\sum_{t\forall s_t=j}$ is better explained intuitively as the fact that changing the parameters of $\theta_j$ will only affect those observations $z_t$ related to state $j$. Setting \cref{eq:jump_derivative} equal to zero and solving for $\theta_j$ yields the solution
\begin{equation}
    \theta_j = \frac{1}{N_j} \sum_{t\forall s_t=j} z_t     
\end{equation}
where $N_j$ is the number of elements in $\mathcal{S}$ that equals $j$. The solution in each state is thus simply a vector of averages from each times series features in $Z$.

Item 2.b in \cref{algo:jump_fit} is minimized recursively in a similar way to the Viterbi algorithm explained earlier, except the time order of operations are reversed. Following the method outlined by Nystrup et al. (2020), $\S^i$ is found using dynamic programming by computing the least costly state sequence through $T$ data points. Define
\begin{align}
    \upsilon_T(s) &= \ell(Z_T, \theta_s) \\
    \upsilon_t(s) &= \ell(Z_T, \theta_s) + \min_j[\upsilon_{t+1}(j) + \lambda\mathbf{1}_{\{ i\ne j \}}],
    \quad t=T-1,\ldots,1
\label{eq:jump_cost_path}
\end{align}
Then, the least costly state sequence is found by
\begin{align}
    s_1 &= \argmin_j \upsilon_1(j) \\
    s_t &= \argmin_j[\upsilon_t(j) + \lambda\mathbf{1}_{\{ s_{t-1 \ne j} \}}], \quad t=2,\ldots,T
    \label{eq:jump_state_seq_path}
\end{align}
$\upsilon$ is thus defined as the least costly path from state $i$ to $j$ between two discrete time steps, as it merely determined by the tradeoff between the jump penalty $\lambda$ and the cost associated with moving to state $j$. Viewing all entries of $\upsilon$, one can easily see how this will represent costs associated with each specific path through $T$ data points in $\upsilon$. \cref{eq:jump_state_seq_path} can therefore be interpreted as the least costly path through $\upsilon$. 

One of the main advantages to the jump model over maximum likelihood estimation is that the jump model fitting is entirely based on squared Euclidean distances and does not assume the states to follow any specific distributions. As such it is expected to be more robust when trained on data in which the underlying distribution(s) are unknown, which is often the case with financial data. The relaxation of this assumption is extremely important since misspecified distributions often are a huge error source in HMMs, as well as other probabilistic times series models. As a result, not being required to make any assumptions about the data's conditional distributions during estimation will potentially significantly improve the model fit. How well this property holds will be tested in a simulation study in \cref{section:simulation_corspeciffied} and \cref{section:simulation_misspecified}. Additionally, as will be shown in section \ref{subsection: jump_penalizer} it is very easy to add more features to the estimator, allowing the use of a variety of both endogenous and exogenous variables. This makes it feasible to potentially test many different features and feature sets when training models, which is no small advantage when pursuing models with highly persistent states.

Finally, the model is computationally lighter to fit and often only requires between 5-10 iterations to converge whereas the Baum-Welch algorithm often requires 50-100 iterations. In the following sections it will be shown how both models converge when trained in a number of simulation environments.